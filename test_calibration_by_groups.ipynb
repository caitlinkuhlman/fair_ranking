{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import FairBoost\n",
    "from FairBoost import FairBoostRegressor\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy import stats\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.cross_validation import *\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "# from sklearn.ensemble import FairBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline\n",
    "%precision %.2f\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "X = pickle.load( open( \"x.p\", \"rb\" ) )\n",
    "y = pickle.load( open( \"y.p\", \"rb\" ) )\n",
    "g = pickle.load( open( \"g.p\", \"rb\" ) )\n",
    "g=np.nan_to_num(g)\n",
    "g0 = np.nan_to_num([i for i,x in enumerate(g) if x==0])\n",
    "g1 = np.nan_to_num([i for i,x in enumerate(g) if x==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# fbt = FairBoostRegressor(DecisionTreeRegressor(max_depth=4),n_estimators=1, random_state=25)\n",
    "# fbt.setGroups(g)\n",
    "\n",
    "# fbt.fit(X,y)\n",
    "# train_pred = fbt.predict(X)\n",
    "# print('train error iteration 1 ', mean_squared_error(train_pred, y))\n",
    "\n",
    "# fbt = FairBoostRegressor(DecisionTreeRegressor(max_depth=4),n_estimators=10, random_state=25)\n",
    "# fbt.setGroups(g)\n",
    "\n",
    "# fbt.fit(X,y)\n",
    "# train_pred = fbt.predict(X)\n",
    "# print('train error iteration 10 ', mean_squared_error(train_pred, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Error metrics\n",
    "\n",
    "#summary of deviation measures - relates to precision/accuracy?\n",
    "# https://en.wikipedia.org/wiki/Deviation_(statistics)#Unsigned_or_absolute_deviation\n",
    "#https://en.wikipedia.org/wiki/Mean_signed_deviation\n",
    "#signed absolute deviation ?\n",
    "# https://en.wikipedia.org/wiki/Average_absolute_deviation\n",
    "#average absolute deviation\n",
    "\n",
    "def get_ae(vals):\n",
    "    return np.sum([math.fabs(x[0]-x[1]) for x in vals])\n",
    "\n",
    "def get_mae(vals):\n",
    "    m = np.sum([math.fabs(x[0]-x[1]) for x in vals])\n",
    "    return m/len(vals)\n",
    "\n",
    "\n",
    "def get_se(vals):\n",
    "    return np.sum([math.pow(x[0]-x[1], 2) for x in vals])\n",
    "    \n",
    "def get_mse(vals):\n",
    "    m = np.sum([math.pow(x[0]-x[1], 2) for x in vals])\n",
    "    return m/len(vals)\n",
    "\n",
    "#overestimate\n",
    "def get_oe(vals):\n",
    "    return np.sum([max(0,x[0]-x[1]) for x in vals])\n",
    "\n",
    "def get_moe(vals):\n",
    "    m = np.sum([max(0,x[0]-x[1]) for x in vals])\n",
    "    return m/len(vals)\n",
    "\n",
    "#underestimate\n",
    "def get_ue(vals):\n",
    "    return np.sum([min(0,x[0]-x[1]) for x in vals])\n",
    "\n",
    "def get_mue(vals):\n",
    "    m = np.sum([min(0,x[0]-x[1]) for x in vals])\n",
    "    return m/len(vals)\n",
    "\n",
    "error_functs = [get_ae, get_mae, get_se, get_mse, get_oe, get_moe, get_ue, get_mue]\n",
    "##### BIN ERRORS: ###########\n",
    "\n",
    "def get_bin_width(data, n):\n",
    "    return (data.max()-data.min())/(n+1)\n",
    "\n",
    "def get_error_binned_eq_depth_by_group(points, nbins, error=get_mse):\n",
    "    mse = []\n",
    "    kf = KFold(len(points), n_folds=nbins, shuffle=True, random_state=1)\n",
    "    for rest, bin in kf:\n",
    "        vals = [points.iloc[i] for i in bin]\n",
    "        mse.append(error(vals))\n",
    "    return mse\n",
    "\n",
    "def plot_binned_error(df, error=get_mse):\n",
    "    indices =np.arange(df.shape[0])\n",
    "    #Calculate optimal width\n",
    "    width = 0.3\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.bar(indices-width,df[df.columns[0]],width,color='b',label='-Ymin')\n",
    "    ax.bar(indices,df[df.columns[1]],width,color='r',label='Ymax')\n",
    "    ax.set_xlim(left=-1,right=len(df))\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_xlabel('Bin')\n",
    "    ax.set_ylabel(error.__name__)\n",
    "    plt.show()\n",
    "    \n",
    "def recomputeBins(y_predict, g0, g1, nbins):\n",
    "        #g0 has indexes of objects in X\n",
    "        # get indexes of sorted predictions for group\n",
    "        sorted0 = np.argsort([y_predict[x] for x in g0])\n",
    "        binSize=int(np.ceil(float(len(g0))/nbins))\n",
    "        bins0=[]\n",
    "        b=[]\n",
    "        i=0\n",
    "        j=binSize-1\n",
    "        for n in range(nbins):\n",
    "            k=int(min(j,len(sorted0)-1))\n",
    "            bins0.append([g0[x] for x in sorted0[i:k]])\n",
    "            i+=binSize\n",
    "            j+=binSize\n",
    "           \n",
    "        #g1 has indexes of objects in X\n",
    "        # get indexes of sorted predictions for group\n",
    "        sorted1 = np.argsort([y_predict[x] for x in g1])\n",
    "        binSize=int(np.ceil(float(len(g1))/nbins))\n",
    "        bins1=[]\n",
    "        b=[]\n",
    "        i=0\n",
    "        j=binSize-1\n",
    "        for n in range(nbins):\n",
    "            k=int(min(j,len(sorted0)-1))\n",
    "            bins1.append([g1[x] for x in sorted1[i:k]])\n",
    "            i+=binSize\n",
    "            j+=binSize\n",
    "        return bins0,bins1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lr = linear_model.LinearRegression()\n",
    "# dt = DecisionTreeRegressor(max_depth=4)\n",
    "# clf_isotonic = CalibratedClassifierCV(clf, cv=2, method='isotonic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(len(X), n_folds=2, shuffle=True, random_state=1)\n",
    "\n",
    "nbins = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.59874933, -0.6160681 ,  1.20615805,  1.66541829, -0.11236331,\n",
       "        1.13208381,  1.13208381, -0.52717902,  0.71726811, -1.16421742,\n",
       "       -1.09014319, -1.13458773, -1.43088466, -1.49014405, -1.26792135,\n",
       "       -1.09014319,  1.42838075, -0.43828994,  0.19874847, -0.64569779,\n",
       "       -0.52717902, -1.43088466,  1.35430651, -1.4753292 ,  0.13948908,\n",
       "       -0.24569693, -1.5049589 ,  0.13948908,  1.42838075,  1.04319473,\n",
       "       -1.32718074,  0.59874933,  1.56171437, -0.43828994, -0.73458687,\n",
       "        1.66541829,  0.43578602, -0.8086611 , -1.34199558,  1.66541829,\n",
       "       -1.60866282,  0.31726724, -0.00865938, -0.11236331,  0.71726811,\n",
       "        0.80615719,  0.06541485, -1.4753292 ,  0.50986025, -0.95680957,\n",
       "        1.04319473,  1.04319473,  1.56171437, -0.11236331, -0.30495632,\n",
       "       -1.56421828,  0.33208209, -0.43828994,  0.19874847,  0.89504627,\n",
       "        1.42838075,  1.13208381,  0.43578602, -1.19384712, -1.04569865,\n",
       "       -1.4753292 ,  1.29504713,  0.50986025, -0.95680957, -1.62347767,\n",
       "       -1.01606896, -1.43088466,  0.9691205 , -0.20125239,  0.89504627,\n",
       "       -0.20125239, -0.24569693,  0.43578602,  0.59874933,  0.19874847,\n",
       "        1.66541829,  1.56171437, -0.73458687, -1.56421828, -0.6160681 ,\n",
       "        1.04319473, -0.73458687,  0.06541485, -0.48273448,  0.9691205 ,\n",
       "        0.33208209, -1.20866196,  0.9691205 ,  0.19874847,  0.33208209,\n",
       "        0.89504627, -1.59384798, -0.8086611 ,  0.19874847, -0.43828994,\n",
       "       -0.34940086, -1.01606896, -0.8382908 ,  0.9691205 ,  1.20615805,\n",
       "        0.89504627, -0.48273448,  0.06541485,  0.33208209,  0.71726811,\n",
       "        0.80615719,  1.04319473, -0.95680957,  0.50986025,  1.20615805,\n",
       "       -1.37162528, -0.11236331, -1.51977374,  0.67282357, -1.13458773,\n",
       "       -0.30495632, -0.00865938,  1.66541829, -1.32718074, -0.34940086,\n",
       "       -0.00865938, -0.20125239,  1.04319473, -0.00865938,  1.56171437,\n",
       "       -1.43088466, -0.95680957,  0.89504627, -1.23829166,  0.43578602,\n",
       "        1.66541829,  1.13208381,  1.42838075,  1.20615805, -0.73458687,\n",
       "       -1.59384798, -0.24569693])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.67567568,  0.30630631,  0.86036036,  1.        ,  0.45945946,\n",
       "        0.83783784,  0.83783784,  0.33333333,  0.71171171,  0.13963964,\n",
       "        0.16216216,  0.14864865,  0.05855856,  0.04054054,  0.10810811,\n",
       "        0.16216216,  0.92792793,  0.36036036,  0.55405405,  0.2972973 ,\n",
       "        0.33333333,  0.05855856,  0.90540541,  0.04504505,  0.53603604,\n",
       "        0.41891892,  0.03603604,  0.53603604,  0.92792793,  0.81081081,\n",
       "        0.09009009,  0.67567568,  0.96846847,  0.36036036,  0.27027027,\n",
       "        1.        ,  0.62612613,  0.24774775,  0.08558559,  1.        ,\n",
       "        0.0045045 ,  0.59009009,  0.49099099,  0.45945946,  0.71171171,\n",
       "        0.73873874,  0.51351351,  0.04504505,  0.64864865,  0.2027027 ,\n",
       "        0.81081081,  0.81081081,  0.96846847,  0.45945946,  0.4009009 ,\n",
       "        0.01801802,  0.59459459,  0.36036036,  0.55405405,  0.76576577,\n",
       "        0.92792793,  0.83783784,  0.62612613,  0.13063063,  0.17567568,\n",
       "        0.04504505,  0.88738739,  0.64864865,  0.2027027 ,  0.        ,\n",
       "        0.18468468,  0.05855856,  0.78828829,  0.43243243,  0.76576577,\n",
       "        0.43243243,  0.41891892,  0.62612613,  0.67567568,  0.55405405,\n",
       "        1.        ,  0.96846847,  0.27027027,  0.01801802,  0.30630631,\n",
       "        0.81081081,  0.27027027,  0.51351351,  0.34684685,  0.78828829,\n",
       "        0.59459459,  0.12612613,  0.78828829,  0.55405405,  0.59459459,\n",
       "        0.76576577,  0.00900901,  0.24774775,  0.55405405,  0.36036036,\n",
       "        0.38738739,  0.18468468,  0.23873874,  0.78828829,  0.86036036,\n",
       "        0.76576577,  0.34684685,  0.51351351,  0.59459459,  0.71171171,\n",
       "        0.73873874,  0.81081081,  0.2027027 ,  0.64864865,  0.86036036,\n",
       "        0.07657658,  0.45945946,  0.03153153,  0.6981982 ,  0.14864865,\n",
       "        0.4009009 ,  0.49099099,  1.        ,  0.09009009,  0.38738739,\n",
       "        0.49099099,  0.43243243,  0.81081081,  0.49099099,  0.96846847,\n",
       "        0.05855856,  0.2027027 ,  0.76576577,  0.11711712,  0.62612613,\n",
       "        1.        ,  0.83783784,  0.92792793,  0.86036036,  0.27027027,\n",
       "        0.00900901,  0.41891892])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y - min(y)) / (max(y)-min(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "continuous target data is not supported with label binarization",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-5bfc91052f42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdt_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclf_isotonic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCalibratedRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"prefit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'isotonic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mclf_isotonic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprob_pos_isotonic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_isotonic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n decision tree error: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-023e362204ca>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mcalibrated_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mcalibrated_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalibrated_classifiers_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalibrated_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-023e362204ca>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_encoder_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_pos_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mlabel_binarize\u001b[0;34m(y, classes, neg_label, pos_label, sparse_output)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         raise ValueError(\"%s target data is not supported with label \"\n\u001b[0;32m--> 519\u001b[0;31m                          \"binarization\" % y_type)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msparse_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: continuous target data is not supported with label binarization"
     ]
    }
   ],
   "source": [
    "for train_index, test_index, in kf:\n",
    "    dt = DecisionTreeRegressor(max_depth=4)\n",
    "    dt.fit(X[train_index],y[train_index])\n",
    "    dt_pred = dt.predict(X[test_index])\n",
    "    clf_isotonic = CalibratedRegressor(dt, cv=\"prefit\", method='isotonic')\n",
    "    clf_isotonic.fit(X[train_index], y[train_index])\n",
    "    prob_pos_isotonic = clf_isotonic.predict_proba(X_test)[:, 1]\n",
    "    print('\\n decision tree error: ', mean_squared_error(dt_pred, y[test_index]))\n",
    "\n",
    "    g0 = [i for i,x in enumerate(g[test_index]) if x==0]\n",
    "    g1 = [i for i,x in enumerate(g[test_index]) if x==1]\n",
    "    bins0,bins1  = recomputeBins(dt_pred, g0, g1, nbins)\n",
    "    error_vect=np.abs(dt_pred - y[test_index])\n",
    "    e0=[np.sum([error_vect[i] for i in b]) for b in bins0]\n",
    "    e1=[np.sum([error_vect[i] for i in b]) for b in bins1]\n",
    "    bin_error = np.subtract(e0, e1)\n",
    "    print(\"dt fair error: \", np.mean(np.abs(bin_error)))\n",
    "\n",
    "    ### PLOTTING ###\n",
    "    results = pd.DataFrame()\n",
    "    results['pred'] = dt_pred\n",
    "    results['y'] = y[test_index]\n",
    "    results['g'] = g[test_index]\n",
    "    results = results.sort_values('pred')\n",
    "    results0 = results[results['g']==0]\n",
    "    print (\"group0: \", results0.shape)\n",
    "    results1 = results[results['g']==1]\n",
    "    print (\"group1: \", results1.shape)\n",
    "\n",
    "    mse_df_depth = pd.DataFrame()\n",
    "    mse_df_depth['mse0'] = get_error_binned_eq_depth_by_group(results0, nbins, error=get_mse)\n",
    "    mse_df_depth['mse1'] = get_error_binned_eq_depth_by_group(results1, nbins, error=get_mse)\n",
    "    plot_binned_error(mse_df_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calibration of predicted probabilities.\"\"\"\n",
    "\n",
    "# Author: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n",
    "#         Balazs Kegl <balazs.kegl@gmail.com>\n",
    "#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n",
    "#         Mathieu Blondel <mathieu@mblondel.org>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import division\n",
    "import warnings\n",
    "\n",
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin, clone\n",
    "from sklearn.preprocessing import label_binarize, LabelBinarizer\n",
    "from sklearn.utils import check_X_y, check_array, indexable, column_or_1d\n",
    "from sklearn.utils.validation import check_is_fitted, check_consistent_length\n",
    "from sklearn.utils.fixes import signature\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import check_cv\n",
    "from sklearn.metrics.classification import _check_binary_probabilistic_predictions\n",
    "\n",
    "\n",
    "class CalibratedRegressor(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Probability calibration with isotonic regression or sigmoid.\n",
    "\n",
    "    With this class, the base_estimator is fit on the train set of the\n",
    "    cross-validation generator and the test set is used for calibration.\n",
    "    The probabilities for each of the folds are then averaged\n",
    "    for prediction. In case that cv=\"prefit\" is passed to __init__,\n",
    "    it is assumed that base_estimator has been fitted already and all\n",
    "    data is used for calibration. Note that data for fitting the\n",
    "    classifier and for calibrating it must be disjoint.\n",
    "\n",
    "    Read more in the :ref:`User Guide <calibration>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator : instance BaseEstimator\n",
    "        The classifier whose output decision function needs to be calibrated\n",
    "        to offer more accurate predict_proba outputs. If cv=prefit, the\n",
    "        classifier must have been fit already on data.\n",
    "\n",
    "    method : 'sigmoid' or 'isotonic'\n",
    "        The method to use for calibration. Can be 'sigmoid' which\n",
    "        corresponds to Platt's method or 'isotonic' which is a\n",
    "        non-parametric approach. It is not advised to use isotonic calibration\n",
    "        with too few calibration samples ``(<<1000)`` since it tends to\n",
    "        overfit.\n",
    "        Use sigmoids (Platt's calibration) in this case.\n",
    "\n",
    "    cv : integer, cross-validation generator, iterable or \"prefit\", optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "        - None, to use the default 3-fold cross-validation,\n",
    "        - integer, to specify the number of folds.\n",
    "        - An object to be used as a cross-validation generator.\n",
    "        - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` is\n",
    "        neither binary nor multiclass, :class:`sklearn.model_selection.KFold`\n",
    "        is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validation strategies that can be used here.\n",
    "\n",
    "        If \"prefit\" is passed, it is assumed that base_estimator has been\n",
    "        fitted already and all data is used for calibration.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : array, shape (n_classes)\n",
    "        The class labels.\n",
    "\n",
    "    calibrated_classifiers_ : list (len() equal to cv or 1 if cv == \"prefit\")\n",
    "        The list of calibrated classifiers, one for each crossvalidation fold,\n",
    "        which has been fitted on all but the validation fold and calibrated\n",
    "        on the validation fold.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Obtaining calibrated probability estimates from decision trees\n",
    "           and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\n",
    "\n",
    "    .. [2] Transforming Classifier Scores into Accurate Multiclass\n",
    "           Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)\n",
    "\n",
    "    .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to\n",
    "           Regularized Likelihood Methods, J. Platt, (1999)\n",
    "\n",
    "    .. [4] Predicting Good Probabilities with Supervised Learning,\n",
    "           A. Niculescu-Mizil & R. Caruana, ICML 2005\n",
    "    \"\"\"\n",
    "    def __init__(self, base_estimator=None, method='sigmoid', cv=3):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.method = method\n",
    "        self.cv = cv\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Fit the calibrated model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values.\n",
    "\n",
    "        sample_weight : array-like, shape = [n_samples] or None\n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "        \"\"\"\n",
    "#         scale the labels to be between 0 and 1\n",
    "        y = (y - min(y)) / (max(y)-min(y))\n",
    "    \n",
    "        X, y = check_X_y(X, y, accept_sparse=['csc', 'csr', 'coo'],\n",
    "                         force_all_finite=False)\n",
    "        X, y = indexable(X, y)\n",
    "#         don't binarize - we will use continuous labels\n",
    "#         le = LabelBinarizer().fit(y)\n",
    "#         self.classes_ = le.classes_\n",
    "\n",
    "        # Check that each cross-validation fold can have at least one\n",
    "        # example per class\n",
    "        n_folds = self.cv if isinstance(self.cv, int) \\\n",
    "            else self.cv.n_folds if hasattr(self.cv, \"n_folds\") else None\n",
    "        if n_folds and \\\n",
    "                np.any([np.sum(y == class_) < n_folds for class_ in\n",
    "                        self.classes_]):\n",
    "            raise ValueError(\"Requesting %d-fold cross-validation but provided\"\n",
    "                             \" less than %d examples for at least one class.\"\n",
    "                             % (n_folds, n_folds))\n",
    "\n",
    "        self.calibrated_classifiers_ = []\n",
    "#         if self.base_estimator is None:\n",
    "#             # we want all classifiers that don't expose a random_state\n",
    "#             # to be deterministic (and we don't want to expose this one).\n",
    "#             base_estimator = LinearSVC(random_state=0)\n",
    "#         else:\n",
    "# enforce that the user has to pass in the base estimator\n",
    "        base_estimator = self.base_estimator\n",
    "\n",
    "        if self.cv == \"prefit\":\n",
    "            calibrated_classifier = _CalibratedClassifier(\n",
    "                base_estimator, method=self.method)\n",
    "            if sample_weight is not None:\n",
    "                calibrated_classifier.fit(X, y, sample_weight)\n",
    "            else:\n",
    "                calibrated_classifier.fit(X, y)\n",
    "            self.calibrated_classifiers_.append(calibrated_classifier)\n",
    "        else:\n",
    "            cv = check_cv(self.cv, y, classifier=True)\n",
    "            fit_parameters = signature(base_estimator.fit).parameters\n",
    "            estimator_name = type(base_estimator).__name__\n",
    "            if (sample_weight is not None\n",
    "                    and \"sample_weight\" not in fit_parameters):\n",
    "                warnings.warn(\"%s does not support sample_weight. Samples\"\n",
    "                              \" weights are only used for the calibration\"\n",
    "                              \" itself.\" % estimator_name)\n",
    "                base_estimator_sample_weight = None\n",
    "            else:\n",
    "                if sample_weight is not None:\n",
    "                    sample_weight = check_array(sample_weight, ensure_2d=False)\n",
    "                    check_consistent_length(y, sample_weight)\n",
    "                base_estimator_sample_weight = sample_weight\n",
    "            for train, test in cv.split(X, y):\n",
    "                this_estimator = clone(base_estimator)\n",
    "                if base_estimator_sample_weight is not None:\n",
    "                    this_estimator.fit(\n",
    "                        X[train], y[train],\n",
    "                        sample_weight=base_estimator_sample_weight[train])\n",
    "                else:\n",
    "                    this_estimator.fit(X[train], y[train])\n",
    "\n",
    "                calibrated_classifier = _CalibratedClassifier(\n",
    "                    this_estimator, method=self.method,\n",
    "                    classes=self.classes_)\n",
    "                if sample_weight is not None:\n",
    "                    calibrated_classifier.fit(X[test], y[test],\n",
    "                                              sample_weight[test])\n",
    "                else:\n",
    "                    calibrated_classifier.fit(X[test], y[test])\n",
    "                self.calibrated_classifiers_.append(calibrated_classifier)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Posterior probabilities of classification\n",
    "\n",
    "        This function returns posterior probabilities of classification\n",
    "        according to each class on an array of test vectors X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array, shape (n_samples, n_classes)\n",
    "            The predicted probas.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"classes_\", \"calibrated_classifiers_\"])\n",
    "        X = check_array(X, accept_sparse=['csc', 'csr', 'coo'],\n",
    "                        force_all_finite=False)\n",
    "        # Compute the arithmetic mean of the predictions of the calibrated\n",
    "        # classifiers\n",
    "        mean_proba = np.zeros((X.shape[0], len(self.classes_)))\n",
    "        for calibrated_classifier in self.calibrated_classifiers_:\n",
    "            proba = calibrated_classifier.predict_proba(X)\n",
    "            mean_proba += proba\n",
    "\n",
    "        mean_proba /= len(self.calibrated_classifiers_)\n",
    "\n",
    "        return mean_proba\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the target of new samples. Can be different from the\n",
    "        prediction of the uncalibrated classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array, shape (n_samples,)\n",
    "            The predicted class.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"classes_\", \"calibrated_classifiers_\"])\n",
    "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
    "\n",
    "\n",
    "class _CalibratedClassifier(object):\n",
    "    \"\"\"Probability calibration with isotonic regression or sigmoid.\n",
    "\n",
    "    It assumes that base_estimator has already been fit, and trains the\n",
    "    calibration on the input set of the fit function. Note that this class\n",
    "    should not be used as an estimator directly. Use CalibratedClassifierCV\n",
    "    with cv=\"prefit\" instead.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator : instance BaseEstimator\n",
    "        The classifier whose output decision function needs to be calibrated\n",
    "        to offer more accurate predict_proba outputs. No default value since\n",
    "        it has to be an already fitted estimator.\n",
    "\n",
    "    method : 'sigmoid' | 'isotonic'\n",
    "        The method to use for calibration. Can be 'sigmoid' which\n",
    "        corresponds to Platt's method or 'isotonic' which is a\n",
    "        non-parametric approach based on isotonic regression.\n",
    "\n",
    "    classes : array-like, shape (n_classes,), optional\n",
    "            Contains unique classes used to fit the base estimator.\n",
    "            if None, then classes is extracted from the given target values\n",
    "            in fit().\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Obtaining calibrated probability estimates from decision trees\n",
    "           and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\n",
    "\n",
    "    .. [2] Transforming Classifier Scores into Accurate Multiclass\n",
    "           Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)\n",
    "\n",
    "    .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to\n",
    "           Regularized Likelihood Methods, J. Platt, (1999)\n",
    "\n",
    "    .. [4] Predicting Good Probabilities with Supervised Learning,\n",
    "           A. Niculescu-Mizil & R. Caruana, ICML 2005\n",
    "    \"\"\"\n",
    "    def __init__(self, base_estimator, method='sigmoid', classes=None):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.method = method\n",
    "        self.classes = classes\n",
    "\n",
    "    def _preproc(self, X):\n",
    "        n_classes = len(self.classes_)\n",
    "        if hasattr(self.base_estimator, \"decision_function\"):\n",
    "            df = self.base_estimator.decision_function(X)\n",
    "            if df.ndim == 1:\n",
    "                df = df[:, np.newaxis]\n",
    "        elif hasattr(self.base_estimator, \"predict_proba\"):\n",
    "            df = self.base_estimator.predict_proba(X)\n",
    "            if n_classes == 2:\n",
    "                df = df[:, 1:]\n",
    "        else:\n",
    "            raise RuntimeError('classifier has no decision_function or '\n",
    "                               'predict_proba method.')\n",
    "\n",
    "        idx_pos_class = self.label_encoder_.\\\n",
    "            transform(self.base_estimator.classes_)\n",
    "\n",
    "        return df, idx_pos_class\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Calibrate the fitted model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values.\n",
    "\n",
    "        sample_weight : array-like, shape = [n_samples] or None\n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "        \"\"\"\n",
    "\n",
    "        self.label_encoder_ = LabelEncoder()\n",
    "        if self.classes is None:\n",
    "            self.label_encoder_.fit(y)\n",
    "        else:\n",
    "            self.label_encoder_.fit(self.classes)\n",
    "\n",
    "        self.classes_ = self.label_encoder_.classes_\n",
    "        Y = label_binarize(y, self.classes_)\n",
    "\n",
    "        df, idx_pos_class = self._preproc(X)\n",
    "        self.calibrators_ = []\n",
    "\n",
    "        for k, this_df in zip(idx_pos_class, df.T):\n",
    "            if self.method == 'isotonic':\n",
    "                calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "            elif self.method == 'sigmoid':\n",
    "                calibrator = _SigmoidCalibration()\n",
    "            else:\n",
    "                raise ValueError('method should be \"sigmoid\" or '\n",
    "                                 '\"isotonic\". Got %s.' % self.method)\n",
    "            calibrator.fit(this_df, Y[:, k], sample_weight)\n",
    "            self.calibrators_.append(calibrator)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Posterior probabilities of classification\n",
    "\n",
    "        This function returns posterior probabilities of classification\n",
    "        according to each class on an array of test vectors X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array, shape (n_samples, n_classes)\n",
    "            The predicted probas. Can be exact zeros.\n",
    "        \"\"\"\n",
    "        n_classes = len(self.classes_)\n",
    "        proba = np.zeros((X.shape[0], n_classes))\n",
    "\n",
    "        df, idx_pos_class = self._preproc(X)\n",
    "\n",
    "        for k, this_df, calibrator in \\\n",
    "                zip(idx_pos_class, df.T, self.calibrators_):\n",
    "            if n_classes == 2:\n",
    "                k += 1\n",
    "            proba[:, k] = calibrator.predict(this_df)\n",
    "\n",
    "        # Normalize the probabilities\n",
    "        if n_classes == 2:\n",
    "            proba[:, 0] = 1. - proba[:, 1]\n",
    "        else:\n",
    "            proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
    "\n",
    "        # XXX : for some reason all probas can be 0\n",
    "        proba[np.isnan(proba)] = 1. / n_classes\n",
    "\n",
    "        # Deal with cases where the predicted probability minimally exceeds 1.0\n",
    "        proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0\n",
    "\n",
    "        return proba\n",
    "\n",
    "\n",
    "def _sigmoid_calibration(df, y, sample_weight=None):\n",
    "    \"\"\"Probability Calibration with sigmoid method (Platt 2000)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : ndarray, shape (n_samples,)\n",
    "        The decision function or predict proba for the samples.\n",
    "\n",
    "    y : ndarray, shape (n_samples,)\n",
    "        The targets.\n",
    "\n",
    "    sample_weight : array-like, shape = [n_samples] or None\n",
    "        Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a : float\n",
    "        The slope.\n",
    "\n",
    "    b : float\n",
    "        The intercept.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Platt, \"Probabilistic Outputs for Support Vector Machines\"\n",
    "    \"\"\"\n",
    "    df = column_or_1d(df)\n",
    "    y = column_or_1d(y)\n",
    "\n",
    "    F = df  # F follows Platt's notations\n",
    "    tiny = np.finfo(np.float).tiny  # to avoid division by 0 warning\n",
    "\n",
    "    # Bayesian priors (see Platt end of section 2.2)\n",
    "    prior0 = float(np.sum(y <= 0))\n",
    "    prior1 = y.shape[0] - prior0\n",
    "    T = np.zeros(y.shape)\n",
    "    T[y > 0] = (prior1 + 1.) / (prior1 + 2.)\n",
    "    T[y <= 0] = 1. / (prior0 + 2.)\n",
    "    T1 = 1. - T\n",
    "\n",
    "    def objective(AB):\n",
    "        # From Platt (beginning of Section 2.2)\n",
    "        E = np.exp(AB[0] * F + AB[1])\n",
    "        P = 1. / (1. + E)\n",
    "        l = -(T * np.log(P + tiny) + T1 * np.log(1. - P + tiny))\n",
    "        if sample_weight is not None:\n",
    "            return (sample_weight * l).sum()\n",
    "        else:\n",
    "            return l.sum()\n",
    "\n",
    "    def grad(AB):\n",
    "        # gradient of the objective function\n",
    "        E = np.exp(AB[0] * F + AB[1])\n",
    "        P = 1. / (1. + E)\n",
    "        TEP_minus_T1P = P * (T * E - T1)\n",
    "        if sample_weight is not None:\n",
    "            TEP_minus_T1P *= sample_weight\n",
    "        dA = np.dot(TEP_minus_T1P, F)\n",
    "        dB = np.sum(TEP_minus_T1P)\n",
    "        return np.array([dA, dB])\n",
    "\n",
    "    AB0 = np.array([0., log((prior0 + 1.) / (prior1 + 1.))])\n",
    "    AB_ = fmin_bfgs(objective, AB0, fprime=grad, disp=False)\n",
    "    return AB_[0], AB_[1]\n",
    "\n",
    "\n",
    "class _SigmoidCalibration(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Sigmoid regression model.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    a_ : float\n",
    "        The slope.\n",
    "\n",
    "    b_ : float\n",
    "        The intercept.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Fit the model using X, y as training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples,)\n",
    "            Training data.\n",
    "\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Training target.\n",
    "\n",
    "        sample_weight : array-like, shape = [n_samples] or None\n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "        \"\"\"\n",
    "        X = column_or_1d(X)\n",
    "        y = column_or_1d(y)\n",
    "        X, y = indexable(X, y)\n",
    "\n",
    "        self.a_, self.b_ = _sigmoid_calibration(X, y, sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, T):\n",
    "        \"\"\"Predict new data by linear interpolation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        T : array-like, shape (n_samples,)\n",
    "            Data to predict from.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        T_ : array, shape (n_samples,)\n",
    "            The predicted data.\n",
    "        \"\"\"\n",
    "        T = column_or_1d(T)\n",
    "        return 1. / (1. + np.exp(self.a_ * T + self.b_))\n",
    "\n",
    "\n",
    "def calibration_curve(y_true, y_prob, normalize=False, n_bins=5):\n",
    "    \"\"\"Compute true and predicted probabilities for a calibration curve.\n",
    "\n",
    "     Calibration curves may also be referred to as reliability diagrams.\n",
    "\n",
    "    Read more in the :ref:`User Guide <calibration>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array, shape (n_samples,)\n",
    "        True targets.\n",
    "\n",
    "    y_prob : array, shape (n_samples,)\n",
    "        Probabilities of the positive class.\n",
    "\n",
    "import warnings\n",
    "    normalize : bool, optional, default=False\n",
    "        Whether y_prob needs to be normalized into the bin [0, 1], i.e. is not\n",
    "        a proper probability. If True, the smallest value in y_prob is mapped\n",
    "        onto 0 and the largest one onto 1.\n",
    "\n",
    "    n_bins : int\n",
    "        Number of bins. A bigger number requires more data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    prob_true : array, shape (n_bins,)\n",
    "        The true probability in each bin (fraction of positives).\n",
    "\n",
    "    prob_pred : array, shape (n_bins,)\n",
    "        The mean predicted probability in each bin.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Alexandru Niculescu-Mizil and Rich Caruana (2005) Predicting Good\n",
    "    Probabilities With Supervised Learning, in Proceedings of the 22nd\n",
    "    International Conference on Machine Learning (ICML).\n",
    "    See section 4 (Qualitative Analysis of Predictions).\n",
    "    \"\"\"\n",
    "    y_true = column_or_1d(y_true)\n",
    "    y_prob = column_or_1d(y_prob)\n",
    "\n",
    "    if normalize:  # Normalize predicted values into interval [0, 1]\n",
    "        y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min())\n",
    "    elif y_prob.min() < 0 or y_prob.max() > 1:\n",
    "        raise ValueError(\"y_prob has values outside [0, 1] and normalize is \"\n",
    "                         \"set to False.\")\n",
    "\n",
    "    y_true = _check_binary_probabilistic_predictions(y_true, y_prob)\n",
    "\n",
    "    bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "    binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "    bin_sums = np.bincount(binids, weights=y_prob, minlength=len(bins))\n",
    "    bin_true = np.bincount(binids, weights=y_true, minlength=len(bins))\n",
    "    bin_total = np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "    nonzero = bin_total != 0\n",
    "    prob_true = (bin_true[nonzero] / bin_total[nonzero])\n",
    "    prob_pred = (bin_sums[nonzero] / bin_total[nonzero])\n",
    "\n",
    "    return prob_true, prob_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-30-4a8b03433d11>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-30-4a8b03433d11>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    TODO:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "TODO:\n",
    "    Not producing any consistent results\n",
    "    Try different ways of updating the weights bin-wise\n",
    "    is error being computed as mse? investigate error_vect\n",
    "    read up on non-differentiable error in boosting. Are we just jumping around?\n",
    "    try plotting change in bin_error over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47    0.78\n",
       "7    -0.40\n",
       "12    0.60\n",
       "69    0.16\n",
       "24    0.16\n",
       "15   -1.32\n",
       "35   -0.63\n",
       "17   -1.58\n",
       "31   -1.32\n",
       "32    0.16\n",
       "10   -1.32\n",
       "8     1.61\n",
       "5    -1.58\n",
       "58    0.16\n",
       "21    0.16\n",
       "36   -1.45\n",
       "42    0.16\n",
       "56   -1.32\n",
       "20    1.19\n",
       "49   -0.63\n",
       "28   -0.96\n",
       "25   -1.32\n",
       "70    1.19\n",
       "6     0.16\n",
       "19   -1.45\n",
       "68   -0.63\n",
       "63    1.19\n",
       "1     1.01\n",
       "48   -0.63\n",
       "43    1.01\n",
       "      ... \n",
       "52    1.01\n",
       "54   -0.96\n",
       "46   -0.40\n",
       "14    0.16\n",
       "39    0.16\n",
       "38    0.16\n",
       "16   -1.58\n",
       "9    -0.40\n",
       "30   -0.63\n",
       "33   -0.40\n",
       "13    1.61\n",
       "2     0.16\n",
       "64    1.19\n",
       "0     0.16\n",
       "55    1.01\n",
       "41   -0.96\n",
       "29    0.16\n",
       "37   -1.32\n",
       "27    0.16\n",
       "67    0.16\n",
       "62    0.16\n",
       "26    1.19\n",
       "22   -0.40\n",
       "65    0.78\n",
       "18    1.19\n",
       "53    1.61\n",
       "51    1.01\n",
       "11   -0.40\n",
       "66   -1.58\n",
       "40   -0.63\n",
       "Name: pred, Length: 71, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
