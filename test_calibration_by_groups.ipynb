{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy to read article about calibration: http://fastml.com/classifier-calibration-with-platts-scaling-and-isotonic-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FairBoost\n",
    "from FairBoost import FairBoostRegressor\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy import stats\n",
    "from sklearn import datasets\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.cross_validation import *\n",
    "#from sklearn.ensemble import AdaBoostRegressor\n",
    "# from sklearn.ensemble import FairBoostRegressor\n",
    "#from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline\n",
    "%precision %.2f\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "X = pickle.load( open( \"x.p\", \"rb\" ) )\n",
    "y = pickle.load( open( \"y.p\", \"rb\" ) )\n",
    "g = pickle.load( open( \"g.p\", \"rb\" ) )\n",
    "g=np.nan_to_num(g)\n",
    "g0 = np.nan_to_num([i for i,x in enumerate(g) if x==0])\n",
    "g1 = np.nan_to_num([i for i,x in enumerate(g) if x==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# fbt = FairBoostRegressor(DecisionTreeRegressor(max_depth=4),n_estimators=1, random_state=25)\n",
    "# fbt.setGroups(g)\n",
    "\n",
    "# fbt.fit(X,y)\n",
    "# train_pred = fbt.predict(X)\n",
    "# print('train error iteration 1 ', mean_squared_error(train_pred, y))\n",
    "\n",
    "# fbt = FairBoostRegressor(DecisionTreeRegressor(max_depth=4),n_estimators=10, random_state=25)\n",
    "# fbt.setGroups(g)\n",
    "\n",
    "# fbt.fit(X,y)\n",
    "# train_pred = fbt.predict(X)\n",
    "# print('train error iteration 10 ', mean_squared_error(train_pred, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Error metrics\n",
    "\n",
    "#summary of deviation measures - relates to precision/accuracy?\n",
    "# https://en.wikipedia.org/wiki/Deviation_(statistics)#Unsigned_or_absolute_deviation\n",
    "#https://en.wikipedia.org/wiki/Mean_signed_deviation\n",
    "#signed absolute deviation ?\n",
    "# https://en.wikipedia.org/wiki/Average_absolute_deviation\n",
    "#average absolute deviation\n",
    "\n",
    "def get_ae(vals):\n",
    "    return np.sum([math.fabs(x[0]-x[1]) for x in vals])\n",
    "\n",
    "def get_mae(vals):\n",
    "    m = np.sum([math.fabs(x[0]-x[1]) for x in vals])\n",
    "    return m/len(vals)\n",
    "\n",
    "\n",
    "def get_se(vals):\n",
    "    return np.sum([math.pow(x[0]-x[1], 2) for x in vals])\n",
    "    \n",
    "def get_mse(vals):\n",
    "    m = np.sum([math.pow(x[0]-x[1], 2) for x in vals])\n",
    "    return m/len(vals)\n",
    "\n",
    "#overestimate\n",
    "def get_oe(vals):\n",
    "    return np.sum([max(0,x[0]-x[1]) for x in vals])\n",
    "\n",
    "def get_moe(vals):\n",
    "    m = np.sum([max(0,x[0]-x[1]) for x in vals])\n",
    "    return m/len(vals)\n",
    "\n",
    "#underestimate\n",
    "def get_ue(vals):\n",
    "    return np.sum([min(0,x[0]-x[1]) for x in vals])\n",
    "\n",
    "def get_mue(vals):\n",
    "    m = np.sum([min(0,x[0]-x[1]) for x in vals])\n",
    "    return m/len(vals)\n",
    "\n",
    "error_functs = [get_ae, get_mae, get_se, get_mse, get_oe, get_moe, get_ue, get_mue]\n",
    "##### BIN ERRORS: ###########\n",
    "\n",
    "def get_bin_width(data, n):\n",
    "    return (data.max()-data.min())/(n+1)\n",
    "\n",
    "def get_error_binned_eq_depth_by_group(points, nbins, error=get_mse):\n",
    "    mse = []\n",
    "    kf = KFold(len(points), n_folds=nbins, shuffle=True, random_state=1)\n",
    "    for rest, bin in kf:\n",
    "        vals = [points.iloc[i] for i in bin]\n",
    "        mse.append(error(vals))\n",
    "    return mse\n",
    "\n",
    "# I guess Python changed the syntax of the KFold function at some point???\n",
    "def get_error_binned_eq_depth_by_group2(points, nbins, error=get_mse):\n",
    "    mse = []\n",
    "    kf = KFold(n_splits=nbins, shuffle=True, random_state=1)\n",
    "    for rest, bin in kf.split(points):\n",
    "        vals = [points.iloc[i] for i in bin]\n",
    "        mse.append(error(vals))\n",
    "    return mse\n",
    "\n",
    "def plot_binned_error(df, error=get_mse):\n",
    "    indices =np.arange(df.shape[0])\n",
    "    #Calculate optimal width\n",
    "    width = 0.3\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.bar(indices-width,df[df.columns[0]],width,color='b',label='-Ymin')\n",
    "    ax.bar(indices,df[df.columns[1]],width,color='r',label='Ymax')\n",
    "    ax.set_xlim(left=-1,right=len(df))\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_xlabel('Bin')\n",
    "    ax.set_ylabel(error.__name__)\n",
    "    plt.show()\n",
    "    \n",
    "def recomputeBins(y_predict, g0, g1, nbins):\n",
    "        #g0 has indexes of objects in X\n",
    "        # get indexes of sorted predictions for group\n",
    "        sorted0 = np.argsort([y_predict[x] for x in g0])\n",
    "        binSize=int(np.ceil(float(len(g0))/nbins))\n",
    "        bins0=[]\n",
    "        b=[]\n",
    "        i=0\n",
    "        j=binSize-1\n",
    "        for n in range(nbins):\n",
    "            k=int(min(j,len(sorted0)-1))\n",
    "            bins0.append([g0[x] for x in sorted0[i:k]])\n",
    "            i+=binSize\n",
    "            j+=binSize\n",
    "           \n",
    "        #g1 has indexes of objects in X\n",
    "        # get indexes of sorted predictions for group\n",
    "        sorted1 = np.argsort([y_predict[x] for x in g1])\n",
    "        binSize=int(np.ceil(float(len(g1))/nbins))\n",
    "        bins1=[]\n",
    "        b=[]\n",
    "        i=0\n",
    "        j=binSize-1\n",
    "        for n in range(nbins):\n",
    "            k=int(min(j,len(sorted0)-1))\n",
    "            bins1.append([g1[x] for x in sorted1[i:k]])\n",
    "            i+=binSize\n",
    "            j+=binSize\n",
    "        return bins0,bins1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lr = linear_model.LinearRegression()\n",
    "# dt = DecisionTreeRegressor(max_depth=4)\n",
    "# clf_isotonic = CalibratedClassifierCV(clf, cv=2, method='isotonic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly assigns observations into train, calibration, and test groups. \n",
    "# Should probably be done with something like KFold\n",
    "#kf = KFold(n_splits=2, shuffle=True, random_state=1)\n",
    "#nbins = 10\n",
    "\n",
    "\n",
    "\n",
    "# X.shape: x is 142 rows with 440 attributes\n",
    "# y.shape: y is 142 rows long\n",
    "\n",
    "# Split data into three groups: train, calibrate, and test\n",
    "# Make a random list of numbers from 1 to X.length \n",
    "from random import shuffle\n",
    "\n",
    "obs = list(range(0,len(X))) #0 to len(X), array positions\n",
    "shuffle(obs)\n",
    "obs\n",
    "\n",
    "#Grab 25% for train, 25% for calibrate, and rest for test\n",
    "import math\n",
    "n_train = math.floor(len(X)*0.25) #25% of the observations are training\n",
    "n_calibrate = math.floor(len(X)*0.25) #25% are calibration\n",
    "\n",
    "#X attributes\n",
    "x_train = X[[obs[0:n_train]]] # training dataset\n",
    "x_calibrate = X[[obs[n_train:n_train+n_calibrate]]] # calibration dataset\n",
    "x_test = X[[obs[n_train+n_calibrate:len(X)]]] # test dataset\n",
    "\n",
    "#assert that split dataset is still the same size as original and no rows were duplicated\n",
    "assert np.vstack({tuple(row) for row in np.r_[x_train,x_calibrate,x_test]}).shape == X.shape\n",
    "\n",
    "# Do the same with y labels\n",
    "y_train = y[obs[0:n_train]] # training dataset\n",
    "y_calibrate = y[obs[n_train:n_train+n_calibrate]] # calibration dataset\n",
    "y_test = y[obs[n_train+n_calibrate:len(y)]] # test dataset\n",
    "assert np.r_[y_train,y_calibrate,y_test].shape == y.shape #same shape, not good idea to check for unique\n",
    "\n",
    "# Do the same thing with g\n",
    "g_train = g[obs[0:n_train]] # training dataset\n",
    "g_calibrate = g[obs[n_train:n_train+n_calibrate]] # calibration dataset\n",
    "g_test = g[obs[n_train+n_calibrate:len(g)]] # test dataset\n",
    "assert np.r_[g_train,g_calibrate,g_test].shape == g.shape #same shape, not good idea to check for unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learn a linear model on the training data\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline using test data\n",
    "y_hat_test_baseline = lr.predict(x_test) # returns y_hat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration model for training isotonic regression\n",
    "y_hat_calibrate = lr.predict(x_calibrate) # returns y_hat_calibrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IsotonicRegression(increasing=True, out_of_bounds='clip', y_max=None,\n",
       "          y_min=None)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit an isotonic regression on y_hat_calibrate and y_calibrate\n",
    "ir = IsotonicRegression( out_of_bounds = 'clip' )\n",
    "ir.fit(y_hat_calibrate, y_calibrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrated error using test data\n",
    "y_hat_test_calibrate = ir.transform(y_hat_test_baseline)   # or ir.fit( p_test ), that's the same thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized\n",
    "# (y - min(y)) / (max(y)-min(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group0:  (41, 3)\n",
      "group1:  (31, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npCYDQpLU9H846x/CMzYq9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1f720f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbins = 10\n",
    "\n",
    "### PLOTTING ###\n",
    "results = pd.DataFrame()\n",
    "results['pred'] = y_hat_test_calibrate\n",
    "results['y'] = y_test\n",
    "results['g'] = g_test\n",
    "results = results.sort_values('pred')\n",
    "results0 = results[results['g']==0]\n",
    "print (\"group0: \", results0.shape)\n",
    "results1 = results[results['g']==1]\n",
    "print (\"group1: \", results1.shape)\n",
    "\n",
    "mse_df_depth = pd.DataFrame()\n",
    "mse_df_depth['mse0'] = get_error_binned_eq_depth_by_group2(results0, nbins, error=get_mse)\n",
    "mse_df_depth['mse1'] = get_error_binned_eq_depth_by_group2(results1, nbins, error=get_mse)\n",
    "plot_binned_error(mse_df_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#kf = KFold(n_splits=2, shuffle=True, random_state=1)\n",
    "\n",
    "#nbins = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train_index, test_index, in kf.split(X):\n",
    "#     dt = DecisionTreeRegressor(max_depth=4)\n",
    "#     dt.fit(X[train_index],y[train_index])\n",
    "#     dt_pred = dt.predict(X[test_index])\n",
    "#     clf_isotonic = CalibratedRegressor(dt, cv=\"prefit\", method='isotonic')\n",
    "#     clf_isotonic.fit(X[train_index], y[train_index])\n",
    "#     prob_pos_isotonic = clf_isotonic.predict_proba(X_test)[:, 1]\n",
    "#     print('\\n decision tree error: ', mean_squared_error(dt_pred, y[test_index]))\n",
    "\n",
    "#     g0 = [i for i,x in enumerate(g[test_index]) if x==0]\n",
    "#     g1 = [i for i,x in enumerate(g[test_index]) if x==1]\n",
    "#     bins0,bins1  = recomputeBins(dt_pred, g0, g1, nbins)\n",
    "#     error_vect=np.abs(dt_pred - y[test_index])\n",
    "#     e0=[np.sum([error_vect[i] for i in b]) for b in bins0]\n",
    "#     e1=[np.sum([error_vect[i] for i in b]) for b in bins1]\n",
    "#     bin_error = np.subtract(e0, e1)\n",
    "#     print(\"dt fair error: \", np.mean(np.abs(bin_error)))\n",
    "\n",
    "#     ### PLOTTING ###\n",
    "#     results = pd.DataFrame()\n",
    "#     results['pred'] = dt_pred\n",
    "#     results['y'] = y[test_index]\n",
    "#     results['g'] = g[test_index]\n",
    "#     results = results.sort_values('pred')\n",
    "#     results0 = results[results['g']==0]\n",
    "#     print (\"group0: \", results0.shape)\n",
    "#     results1 = results[results['g']==1]\n",
    "#     print (\"group1: \", results1.shape)\n",
    "\n",
    "#     mse_df_depth = pd.DataFrame()\n",
    "#     mse_df_depth['mse0'] = get_error_binned_eq_depth_by_group(results0, nbins, error=get_mse)\n",
    "#     mse_df_depth['mse1'] = get_error_binned_eq_depth_by_group(results1, nbins, error=get_mse)\n",
    "#     plot_binned_error(mse_df_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "    Not producing any consistent results\n",
    "    Try different ways of updating the weights bin-wise\n",
    "    is error being computed as mse? investigate error_vect\n",
    "    read up on non-differentiable error in boosting. Are we just jumping around?\n",
    "    try plotting change in bin_error over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
