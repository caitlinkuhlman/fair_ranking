{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Calibration of predicted probabilities.\"\"\"\n",
    "\n",
    "# Author: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n",
    "#         Balazs Kegl <balazs.kegl@gmail.com>\n",
    "#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n",
    "#         Mathieu Blondel <mathieu@mblondel.org>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import division\n",
    "import warnings\n",
    "\n",
    "from math import log\n",
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin, clone\n",
    "from sklearn.preprocessing import label_binarize, LabelBinarizer\n",
    "from sklearn.utils import check_X_y, check_array, indexable, column_or_1d\n",
    "from sklearn.utils.validation import check_is_fitted, check_consistent_length\n",
    "from sklearn.utils.fixes import signature\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import check_cv\n",
    "from sklearn.metrics.classification import _check_binary_probabilistic_predictions\n",
    "\n",
    "\n",
    "class CalibratedRegressor(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"Probability calibration with isotonic regression or sigmoid.\n",
    "\n",
    "    With this class, the base_estimator is fit on the train set of the\n",
    "    cross-validation generator and the test set is used for calibration.\n",
    "    The probabilities for each of the folds are then averaged\n",
    "    for prediction. In case that cv=\"prefit\" is passed to __init__,\n",
    "    it is assumed that base_estimator has been fitted already and all\n",
    "    data is used for calibration. Note that data for fitting the\n",
    "    classifier and for calibrating it must be disjoint.\n",
    "\n",
    "    Read more in the :ref:`User Guide <calibration>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator : instance BaseEstimator\n",
    "        The classifier whose output decision function needs to be calibrated\n",
    "        to offer more accurate predict_proba outputs. If cv=prefit, the\n",
    "        classifier must have been fit already on data.\n",
    "\n",
    "    method : 'sigmoid' or 'isotonic'\n",
    "        The method to use for calibration. Can be 'sigmoid' which\n",
    "        corresponds to Platt's method or 'isotonic' which is a\n",
    "        non-parametric approach. It is not advised to use isotonic calibration\n",
    "        with too few calibration samples ``(<<1000)`` since it tends to\n",
    "        overfit.\n",
    "        Use sigmoids (Platt's calibration) in this case.\n",
    "\n",
    "    cv : integer, cross-validation generator, iterable or \"prefit\", optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "        - None, to use the default 3-fold cross-validation,\n",
    "        - integer, to specify the number of folds.\n",
    "        - An object to be used as a cross-validation generator.\n",
    "        - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` is\n",
    "        neither binary nor multiclass, :class:`sklearn.model_selection.KFold`\n",
    "        is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validation strategies that can be used here.\n",
    "\n",
    "        If \"prefit\" is passed, it is assumed that base_estimator has been\n",
    "        fitted already and all data is used for calibration.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : array, shape (n_classes)\n",
    "        The class labels.\n",
    "\n",
    "    calibrated_classifiers_ : list (len() equal to cv or 1 if cv == \"prefit\")\n",
    "        The list of calibrated classifiers, one for each crossvalidation fold,\n",
    "        which has been fitted on all but the validation fold and calibrated\n",
    "        on the validation fold.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Obtaining calibrated probability estimates from decision trees\n",
    "           and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\n",
    "\n",
    "    .. [2] Transforming Classifier Scores into Accurate Multiclass\n",
    "           Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)\n",
    "\n",
    "    .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to\n",
    "           Regularized Likelihood Methods, J. Platt, (1999)\n",
    "\n",
    "    .. [4] Predicting Good Probabilities with Supervised Learning,\n",
    "           A. Niculescu-Mizil & R. Caruana, ICML 2005\n",
    "    \"\"\"\n",
    "    def __init__(self, base_estimator=None, method='sigmoid', cv=3):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.method = method\n",
    "        self.cv = cv\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Fit the calibrated model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values.\n",
    "\n",
    "        sample_weight : array-like, shape = [n_samples] or None\n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "        \"\"\"\n",
    "#         scale the labels to be between 0 and 1\n",
    "        y = (y - min(y)) / (max(y)-min(y))\n",
    "    \n",
    "        X, y = check_X_y(X, y, accept_sparse=['csc', 'csr', 'coo'],\n",
    "                         force_all_finite=False)\n",
    "        X, y = indexable(X, y)\n",
    "#         don't binarize - we will use continuous labels\n",
    "#         le = LabelBinarizer().fit(y)\n",
    "#         self.classes_ = le.classes_\n",
    "\n",
    "        # Check that each cross-validation fold can have at least one\n",
    "        # example per class\n",
    "        n_folds = self.cv if isinstance(self.cv, int) \\\n",
    "            else self.cv.n_folds if hasattr(self.cv, \"n_folds\") else None\n",
    "        if n_folds and \\\n",
    "                np.any([np.sum(y == class_) < n_folds for class_ in\n",
    "                        self.classes_]):\n",
    "            raise ValueError(\"Requesting %d-fold cross-validation but provided\"\n",
    "                             \" less than %d examples for at least one class.\"\n",
    "                             % (n_folds, n_folds))\n",
    "\n",
    "        self.calibrated_classifiers_ = []\n",
    "#         if self.base_estimator is None:\n",
    "#             # we want all classifiers that don't expose a random_state\n",
    "#             # to be deterministic (and we don't want to expose this one).\n",
    "#             base_estimator = LinearSVC(random_state=0)\n",
    "#         else:\n",
    "# enforce that the user has to pass in the base estimator\n",
    "        base_estimator = self.base_estimator\n",
    "\n",
    "        if self.cv == \"prefit\":\n",
    "            calibrated_classifier = _CalibratedClassifier(\n",
    "                base_estimator, method=self.method)\n",
    "            if sample_weight is not None:\n",
    "                calibrated_classifier.fit(X, y, sample_weight)\n",
    "            else:\n",
    "                calibrated_classifier.fit(X, y)\n",
    "            self.calibrated_classifiers_.append(calibrated_classifier)\n",
    "        else:\n",
    "            cv = check_cv(self.cv, y, classifier=True)\n",
    "            fit_parameters = signature(base_estimator.fit).parameters\n",
    "            estimator_name = type(base_estimator).__name__\n",
    "            if (sample_weight is not None\n",
    "                    and \"sample_weight\" not in fit_parameters):\n",
    "                warnings.warn(\"%s does not support sample_weight. Samples\"\n",
    "                              \" weights are only used for the calibration\"\n",
    "                              \" itself.\" % estimator_name)\n",
    "                base_estimator_sample_weight = None\n",
    "            else:\n",
    "                if sample_weight is not None:\n",
    "                    sample_weight = check_array(sample_weight, ensure_2d=False)\n",
    "                    check_consistent_length(y, sample_weight)\n",
    "                base_estimator_sample_weight = sample_weight\n",
    "            for train, test in cv.split(X, y):\n",
    "                this_estimator = clone(base_estimator)\n",
    "                if base_estimator_sample_weight is not None:\n",
    "                    this_estimator.fit(\n",
    "                        X[train], y[train],\n",
    "                        sample_weight=base_estimator_sample_weight[train])\n",
    "                else:\n",
    "                    this_estimator.fit(X[train], y[train])\n",
    "\n",
    "                calibrated_classifier = _CalibratedClassifier(\n",
    "                    this_estimator, method=self.method,\n",
    "                    classes=self.classes_)\n",
    "                if sample_weight is not None:\n",
    "                    calibrated_classifier.fit(X[test], y[test],\n",
    "                                              sample_weight[test])\n",
    "                else:\n",
    "                    calibrated_classifier.fit(X[test], y[test])\n",
    "                self.calibrated_classifiers_.append(calibrated_classifier)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Posterior probabilities of classification\n",
    "\n",
    "        This function returns posterior probabilities of classification\n",
    "        according to each class on an array of test vectors X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array, shape (n_samples, n_classes)\n",
    "            The predicted probas.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"classes_\", \"calibrated_classifiers_\"])\n",
    "        X = check_array(X, accept_sparse=['csc', 'csr', 'coo'],\n",
    "                        force_all_finite=False)\n",
    "        # Compute the arithmetic mean of the predictions of the calibrated\n",
    "        # classifiers\n",
    "        mean_proba = np.zeros((X.shape[0], len(self.classes_)))\n",
    "        for calibrated_classifier in self.calibrated_classifiers_:\n",
    "            proba = calibrated_classifier.predict_proba(X)\n",
    "            mean_proba += proba\n",
    "\n",
    "        mean_proba /= len(self.calibrated_classifiers_)\n",
    "\n",
    "        return mean_proba\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the target of new samples. Can be different from the\n",
    "        prediction of the uncalibrated classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array, shape (n_samples,)\n",
    "            The predicted class.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, [\"classes_\", \"calibrated_classifiers_\"])\n",
    "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
    "\n",
    "\n",
    "class _CalibratedClassifier(object):\n",
    "    \"\"\"Probability calibration with isotonic regression or sigmoid.\n",
    "\n",
    "    It assumes that base_estimator has already been fit, and trains the\n",
    "    calibration on the input set of the fit function. Note that this class\n",
    "    should not be used as an estimator directly. Use CalibratedClassifierCV\n",
    "    with cv=\"prefit\" instead.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator : instance BaseEstimator\n",
    "        The classifier whose output decision function needs to be calibrated\n",
    "        to offer more accurate predict_proba outputs. No default value since\n",
    "        it has to be an already fitted estimator.\n",
    "\n",
    "    method : 'sigmoid' | 'isotonic'\n",
    "        The method to use for calibration. Can be 'sigmoid' which\n",
    "        corresponds to Platt's method or 'isotonic' which is a\n",
    "        non-parametric approach based on isotonic regression.\n",
    "\n",
    "    classes : array-like, shape (n_classes,), optional\n",
    "            Contains unique classes used to fit the base estimator.\n",
    "            if None, then classes is extracted from the given target values\n",
    "            in fit().\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Obtaining calibrated probability estimates from decision trees\n",
    "           and naive Bayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\n",
    "\n",
    "    .. [2] Transforming Classifier Scores into Accurate Multiclass\n",
    "           Probability Estimates, B. Zadrozny & C. Elkan, (KDD 2002)\n",
    "\n",
    "    .. [3] Probabilistic Outputs for Support Vector Machines and Comparisons to\n",
    "           Regularized Likelihood Methods, J. Platt, (1999)\n",
    "\n",
    "    .. [4] Predicting Good Probabilities with Supervised Learning,\n",
    "           A. Niculescu-Mizil & R. Caruana, ICML 2005\n",
    "    \"\"\"\n",
    "    def __init__(self, base_estimator, method='sigmoid', classes=None):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.method = method\n",
    "        self.classes = classes\n",
    "\n",
    "    def _preproc(self, X):\n",
    "        n_classes = len(self.classes_)\n",
    "        if hasattr(self.base_estimator, \"decision_function\"):\n",
    "            df = self.base_estimator.decision_function(X)\n",
    "            if df.ndim == 1:\n",
    "                df = df[:, np.newaxis]\n",
    "        #elif hasattr(self.base_estimator, \"predict_proba\"):\n",
    "        else:\n",
    "            df = self.base_estimator.predict_proba(X)\n",
    "            if n_classes == 2:\n",
    "                df = df[:, 1:]\n",
    "   #     else:\n",
    "   #         raise RuntimeError('classifier has no decision_function or '\n",
    "   #                            'predict_proba method.')\n",
    "\n",
    "        idx_pos_class = self.label_encoder_.\\\n",
    "            transform(self.base_estimator.classes_)\n",
    "\n",
    "        return df, idx_pos_class\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Calibrate the fitted model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values.\n",
    "\n",
    "        sample_weight : array-like, shape = [n_samples] or None\n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "        \"\"\"\n",
    "\n",
    "        self.label_encoder_ = LabelEncoder()\n",
    "        if self.classes is None:\n",
    "            self.label_encoder_.fit(y)\n",
    "        else:\n",
    "            self.label_encoder_.fit(self.classes)\n",
    "\n",
    "        self.classes_ = self.label_encoder_.classes_\n",
    "        #Y = label_binarize(y, self.classes_)\n",
    "\n",
    "        df, idx_pos_class = self._preproc(X)\n",
    "        self.calibrators_ = []\n",
    "\n",
    "        for k, this_df in zip(idx_pos_class, df.T):\n",
    "            if self.method == 'isotonic':\n",
    "                calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "            elif self.method == 'sigmoid':\n",
    "                calibrator = _SigmoidCalibration()\n",
    "            else:\n",
    "                raise ValueError('method should be \"sigmoid\" or '\n",
    "                                 '\"isotonic\". Got %s.' % self.method)\n",
    "            calibrator.fit(this_df, y[:, k], sample_weight)\n",
    "            self.calibrators_.append(calibrator)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Posterior probabilities of classification\n",
    "\n",
    "        This function returns posterior probabilities of classification\n",
    "        according to each class on an array of test vectors X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C : array, shape (n_samples, n_classes)\n",
    "            The predicted probas. Can be exact zeros.\n",
    "        \"\"\"\n",
    "        n_classes = len(self.classes_)\n",
    "        proba = np.zeros((X.shape[0], n_classes))\n",
    "\n",
    "        df, idx_pos_class = self._preproc(X)\n",
    "\n",
    "        for k, this_df, calibrator in \\\n",
    "                zip(idx_pos_class, df.T, self.calibrators_):\n",
    "            if n_classes == 2:\n",
    "                k += 1\n",
    "            proba[:, k] = calibrator.predict(this_df)\n",
    "\n",
    "        # Normalize the probabilities\n",
    "        if n_classes == 2:\n",
    "            proba[:, 0] = 1. - proba[:, 1]\n",
    "        else:\n",
    "            proba /= np.sum(proba, axis=1)[:, np.newaxis]\n",
    "\n",
    "        # XXX : for some reason all probas can be 0\n",
    "        proba[np.isnan(proba)] = 1. / n_classes\n",
    "\n",
    "        # Deal with cases where the predicted probability minimally exceeds 1.0\n",
    "        proba[(1.0 < proba) & (proba <= 1.0 + 1e-5)] = 1.0\n",
    "\n",
    "        return proba\n",
    "\n",
    "\n",
    "def _sigmoid_calibration(df, y, sample_weight=None):\n",
    "    \"\"\"Probability Calibration with sigmoid method (Platt 2000)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : ndarray, shape (n_samples,)\n",
    "        The decision function or predict proba for the samples.\n",
    "\n",
    "    y : ndarray, shape (n_samples,)\n",
    "        The targets.\n",
    "\n",
    "    sample_weight : array-like, shape = [n_samples] or None\n",
    "        Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a : float\n",
    "        The slope.\n",
    "\n",
    "    b : float\n",
    "        The intercept.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Platt, \"Probabilistic Outputs for Support Vector Machines\"\n",
    "    \"\"\"\n",
    "    df = column_or_1d(df)\n",
    "    y = column_or_1d(y)\n",
    "\n",
    "    F = df  # F follows Platt's notations\n",
    "    tiny = np.finfo(np.float).tiny  # to avoid division by 0 warning\n",
    "\n",
    "    # Bayesian priors (see Platt end of section 2.2)\n",
    "    prior0 = float(np.sum(y <= 0))\n",
    "    prior1 = y.shape[0] - prior0\n",
    "    T = np.zeros(y.shape)\n",
    "    T[y > 0] = (prior1 + 1.) / (prior1 + 2.)\n",
    "    T[y <= 0] = 1. / (prior0 + 2.)\n",
    "    T1 = 1. - T\n",
    "\n",
    "    def objective(AB):\n",
    "        # From Platt (beginning of Section 2.2)\n",
    "        E = np.exp(AB[0] * F + AB[1])\n",
    "        P = 1. / (1. + E)\n",
    "        l = -(T * np.log(P + tiny) + T1 * np.log(1. - P + tiny))\n",
    "        if sample_weight is not None:\n",
    "            return (sample_weight * l).sum()\n",
    "        else:\n",
    "            return l.sum()\n",
    "\n",
    "    def grad(AB):\n",
    "        # gradient of the objective function\n",
    "        E = np.exp(AB[0] * F + AB[1])\n",
    "        P = 1. / (1. + E)\n",
    "        TEP_minus_T1P = P * (T * E - T1)\n",
    "        if sample_weight is not None:\n",
    "            TEP_minus_T1P *= sample_weight\n",
    "        dA = np.dot(TEP_minus_T1P, F)\n",
    "        dB = np.sum(TEP_minus_T1P)\n",
    "        return np.array([dA, dB])\n",
    "\n",
    "    AB0 = np.array([0., log((prior0 + 1.) / (prior1 + 1.))])\n",
    "    AB_ = fmin_bfgs(objective, AB0, fprime=grad, disp=False)\n",
    "    return AB_[0], AB_[1]\n",
    "\n",
    "\n",
    "class _SigmoidCalibration(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Sigmoid regression model.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    a_ : float\n",
    "        The slope.\n",
    "\n",
    "    b_ : float\n",
    "        The intercept.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Fit the model using X, y as training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples,)\n",
    "            Training data.\n",
    "\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Training target.\n",
    "\n",
    "        sample_weight : array-like, shape = [n_samples] or None\n",
    "            Sample weights. If None, then samples are equally weighted.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns an instance of self.\n",
    "        \"\"\"\n",
    "        X = column_or_1d(X)\n",
    "        y = column_or_1d(y)\n",
    "        X, y = indexable(X, y)\n",
    "\n",
    "        self.a_, self.b_ = _sigmoid_calibration(X, y, sample_weight)\n",
    "        return self\n",
    "\n",
    "    def predict(self, T):\n",
    "        \"\"\"Predict new data by linear interpolation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        T : array-like, shape (n_samples,)\n",
    "            Data to predict from.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        T_ : array, shape (n_samples,)\n",
    "            The predicted data.\n",
    "        \"\"\"\n",
    "        T = column_or_1d(T)\n",
    "        return 1. / (1. + np.exp(self.a_ * T + self.b_))\n",
    "\n",
    "\n",
    "def calibration_curve(y_true, y_prob, normalize=False, n_bins=5):\n",
    "    \"\"\"Compute true and predicted probabilities for a calibration curve.\n",
    "\n",
    "     Calibration curves may also be referred to as reliability diagrams.\n",
    "\n",
    "    Read more in the :ref:`User Guide <calibration>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array, shape (n_samples,)\n",
    "        True targets.\n",
    "\n",
    "    y_prob : array, shape (n_samples,)\n",
    "        Probabilities of the positive class.\n",
    "\n",
    "import warnings\n",
    "    normalize : bool, optional, default=False\n",
    "        Whether y_prob needs to be normalized into the bin [0, 1], i.e. is not\n",
    "        a proper probability. If True, the smallest value in y_prob is mapped\n",
    "        onto 0 and the largest one onto 1.\n",
    "\n",
    "    n_bins : int\n",
    "        Number of bins. A bigger number requires more data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    prob_true : array, shape (n_bins,)\n",
    "        The true probability in each bin (fraction of positives).\n",
    "\n",
    "    prob_pred : array, shape (n_bins,)\n",
    "        The mean predicted probability in each bin.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Alexandru Niculescu-Mizil and Rich Caruana (2005) Predicting Good\n",
    "    Probabilities With Supervised Learning, in Proceedings of the 22nd\n",
    "    International Conference on Machine Learning (ICML).\n",
    "    See section 4 (Qualitative Analysis of Predictions).\n",
    "    \"\"\"\n",
    "    y_true = column_or_1d(y_true)\n",
    "    y_prob = column_or_1d(y_prob)\n",
    "\n",
    "    if normalize:  # Normalize predicted values into interval [0, 1]\n",
    "        y_prob = (y_prob - y_prob.min()) / (y_prob.max() - y_prob.min())\n",
    "    elif y_prob.min() < 0 or y_prob.max() > 1:\n",
    "        raise ValueError(\"y_prob has values outside [0, 1] and normalize is \"\n",
    "                         \"set to False.\")\n",
    "\n",
    "    y_true = _check_binary_probabilistic_predictions(y_true, y_prob)\n",
    "\n",
    "    bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "    binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "    bin_sums = np.bincount(binids, weights=y_prob, minlength=len(bins))\n",
    "    bin_true = np.bincount(binids, weights=y_true, minlength=len(bins))\n",
    "    bin_total = np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "    nonzero = bin_total != 0\n",
    "    prob_true = (bin_true[nonzero] / bin_total[nonzero])\n",
    "    prob_pred = (bin_sums[nonzero] / bin_total[nonzero])\n",
    "\n",
    "    return prob_true, prob_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
