{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Weight Boosting\n",
    "This module contains weight boosting estimators for both classification and\n",
    "regression.\n",
    "The module structure is the following:\n",
    "- The ``BaseWeightBoosting`` base class implements a common ``fit`` method\n",
    "  for all the estimators in the module. Regression and classification\n",
    "  only differ from each other in the loss function that is optimized.\n",
    "- ``AdaBoostClassifier`` implements adaptive boosting (AdaBoost-SAMME) for\n",
    "  classification problems.\n",
    "- ``AdaBoostRegressor`` implements adaptive boosting (AdaBoost.R2) for\n",
    "  regression problems.\n",
    "\"\"\"\n",
    "\n",
    "# Authors: Noel Dawe <noel@dawe.me>\n",
    "#          Gilles Louppe <g.louppe@gmail.com>\n",
    "#          Hamzeh Alsalhi <ha258@cornell.edu>\n",
    "#          Arnaud Joly <arnaud.v.joly@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from numpy.core.umath_tests import inner1d\n",
    "from sklearn.ensemble.base import BaseEnsemble\n",
    "from sklearn.base import ClassifierMixin, RegressorMixin, is_regressor, is_classifier\n",
    "from sklearn.externals import six\n",
    "from sklearn.externals.six.moves import zip\n",
    "from sklearn.externals.six.moves import xrange as range\n",
    "from sklearn.ensemble.forest import BaseForest\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.tree.tree import BaseDecisionTree\n",
    "from sklearn.tree._tree import DTYPE\n",
    "from sklearn.utils import check_array, check_X_y, check_random_state\n",
    "from sklearn.utils import extmath\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.utils.validation import has_fit_parameter, check_is_fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/ensemble/weight_boosting.py#L853"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stable_cumsum(arr, axis=None, rtol=1e-05, atol=1e-08):\n",
    "    \"\"\"Use high precision for cumsum and check that final value matches sum\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : array-like\n",
    "        To be cumulatively summed as flat\n",
    "    axis : int, optional\n",
    "        Axis along which the cumulative sum is computed.\n",
    "        The default (None) is to compute the cumsum over the flattened array.\n",
    "    rtol : float\n",
    "        Relative tolerance, see ``np.allclose``\n",
    "    atol : float\n",
    "        Absolute tolerance, see ``np.allclose``\n",
    "    \"\"\"\n",
    "\n",
    "    out = np.cumsum(arr, axis=axis, dtype=np.float64)\n",
    "    expected = np.sum(arr, axis=axis, dtype=np.float64)\n",
    "    if not np.all(np.isclose(out.take(-1, axis=axis), expected, rtol=rtol,\n",
    "                             atol=atol, equal_nan=True)):\n",
    "        warnings.warn('cumsum was found to be unstable: '\n",
    "                      'its last element does not correspond to sum',\n",
    "                      RuntimeWarning)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caitlin/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MSE:  0.150724636201\n",
      "Overall MSE:  0.0715920773439\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=4)\n",
    "abt = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),n_estimators=300, random_state=25)\n",
    "\n",
    "pred_dt = cross_val_predict(dt, X, y, cv=10)\n",
    "pred_abt = cross_val_predict(abt, X, y, cv=10)\n",
    "print(\"Overall MSE: \",  mean_squared_error(y, pred_dt))\n",
    "print(\"Overall MSE: \",  mean_squared_error(y, pred_abt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def recomputeBins(y_predict, g0, g1, nbins):\n",
    "        #g0 has indexes of objects in X\n",
    "        # get indexes of sorted predictions for group\n",
    "        sorted0 = np.argsort([y_predict[x] for x in g0])\n",
    "        binSize=int(np.ceil(float(len(g0))/nbins))\n",
    "        bins0=[]\n",
    "        b=[]\n",
    "        i=0\n",
    "        j=binSize-1\n",
    "        for n in range(nbins):\n",
    "            k=int(min(j,len(sorted0)-1))\n",
    "            bins0.append([g0[x] for x in sorted0[i:k]])\n",
    "            i+=binSize\n",
    "            j+=binSize\n",
    "           \n",
    "        #g1 has indexes of objects in X\n",
    "        # get indexes of sorted predictions for group\n",
    "        sorted1 = np.argsort([y_predict[x] for x in g1])\n",
    "        binSize=int(np.ceil(float(len(g1))/nbins))\n",
    "        bins1=[]\n",
    "        b=[]\n",
    "        i=0\n",
    "        j=binSize-1\n",
    "        for n in range(nbins):\n",
    "            k=int(min(j,len(sorted0)-1))\n",
    "            bins1.append([g1[x] for x in sorted1[i:k]])\n",
    "            i+=binSize\n",
    "            j+=binSize\n",
    "        return bins0,bins1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = DecisionTreeRegressor(max_depth=4)\n",
    "# Initialize weights to 1 / n_samples\n",
    "sample_weight = np.empty(X.shape[0], dtype=np.float64)\n",
    "sample_weight[:] = 1. / X.shape[0]        \n",
    "cdf = stable_cumsum(sample_weight)\n",
    "cdf /= cdf[-1]\n",
    "uniform_samples = np.random.random_sample(X.shape[0])\n",
    "bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')\n",
    "# searchsorted returns a scalar\n",
    "bootstrap_idx = np.array(bootstrap_idx, copy=False)\n",
    "\n",
    "# Fit on the bootstrapped sample and obtain a prediction\n",
    "# for all samples in the training set\n",
    "estimator.fit(X[bootstrap_idx], y[bootstrap_idx])\n",
    "y_predict = estimator.predict(X)\n",
    "error_vect = np.abs(y_predict - y)\n",
    "error_max = error_vect.max()\n",
    "if error_max != 0.:\n",
    "    error_vect /= error_max\n",
    "    \n",
    "# ################ compute bin-wise group error #####################\n",
    "\n",
    "nbins=10\n",
    "bins0,bins1  = recomputeBins(y_predict, g0, g1, nbins)\n",
    "# sum the error for items in bin for each group\n",
    "e0=[np.sum([error_vect[i] for i in b]) for b in bins0]\n",
    "e1=[np.sum([error_vect[i] for i in b]) for b in bins1]\n",
    "\n",
    "bin_error = np.subtract(e0, e1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ################# Update weights based on binned error ############\n",
    "\n",
    "# # weight proportional to difference between the error in each group in the bin. \n",
    "# # but could be based off of the individual error for each term\n",
    "# # or some combination\n",
    "# for i,e in enumerate(bin_error):\n",
    "#     if e < 0:\n",
    "#         for x in bins0[i]:\n",
    "#             error_vect[x]=bin_error[i]\n",
    "#         for x in bins1[i]:\n",
    "#             error_vect[x]=0\n",
    "#     else:\n",
    "#         for x in bins1[i]:\n",
    "#             error_vect[x]=bin_error[i]\n",
    "#         for x in bins0[i]:\n",
    "#             error_vect[x]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ADAPTED BOOST METHOD\n",
    "\n",
    "\n",
    "# def _boost(self, iboost, X, y, sample_weight, random_state):\n",
    "#         \"\"\"Implement a single boost for regression\n",
    "#         Perform a single boost according to the AdaBoost.R2 algorithm and\n",
    "#         return the updated sample weights.\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         iboost : int\n",
    "#             The index of the current boost iteration.\n",
    "#         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "#             The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "#             DOK, or LIL. DOK and LIL are converted to CSR.\n",
    "#         y : array-like of shape = [n_samples]\n",
    "#             The target values (class labels in classification, real numbers in\n",
    "#             regression).\n",
    "#         sample_weight : array-like of shape = [n_samples]\n",
    "#             The current sample weights.\n",
    "#         random_state : numpy.RandomState\n",
    "#             The current random number generator\n",
    "#         Returns\n",
    "#         -------\n",
    "#         sample_weight : array-like of shape = [n_samples] or None\n",
    "#             The reweighted sample weights.\n",
    "#             If None then boosting has terminated early.\n",
    "#         estimator_weight : float\n",
    "#             The weight for the current boost.\n",
    "#             If None then boosting has terminated early.\n",
    "#         estimator_error : float\n",
    "#             The regression error for the current boost.\n",
    "#             If None then boosting has terminated early.\n",
    "#         \"\"\"\n",
    "#         estimator = self._make_estimator(random_state=random_state)\n",
    "        \n",
    "#         # Weighted sampling of the training set with replacement\n",
    "#         # For NumPy >= 1.7.0 use np.random.choice\n",
    "#         cdf = stable_cumsum(sample_weight)\n",
    "#         cdf /= cdf[-1]\n",
    "#         uniform_samples = random_state.random_sample(X.shape[0])\n",
    "#         bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')\n",
    "#         # searchsorted returns a scalar\n",
    "#         bootstrap_idx = np.array(bootstrap_idx, copy=False)\n",
    "\n",
    "#         # Fit on the bootstrapped sample and obtain a prediction\n",
    "#         # for all samples in the training set\n",
    "#         estimator.fit(X[bootstrap_idx], y[bootstrap_idx])\n",
    "#         y_predict = estimator.predict(X)\n",
    "\n",
    "# ###########  REPLACE ERROR VECT WITH OUR OWN   ########\n",
    "# #         error_vect = np.abs(y_predict - y)\n",
    "    \n",
    "# ################ compute bin-wise group error #####################\n",
    "\n",
    "#         nbins=10\n",
    "#         bins0,bins1  = recomputeBins(y_predict, g0, g1, nbins)\n",
    "#         # sum the error for items in bin for each group\n",
    "#         e0=[np.sum([error_vect[i] for i in b]) for b in bins0]\n",
    "#         e1=[np.sum([error_vect[i] for i in b]) for b in bins1]\n",
    "\n",
    "#         bin_error = np.subtract(e0, e1)\n",
    "\n",
    "#         ################# Update weights based on binned error ############\n",
    "\n",
    "#         # weight proportional to difference between the error in each group in the bin. \n",
    "#         # but could be based off of the individual error for each term\n",
    "#         # or some combination\n",
    "#         for i,e in enumerate(bin_error):\n",
    "#             if e < 0:\n",
    "#                 for x in bins0[i]:\n",
    "#                     error_vect[x]=bin_error[i]\n",
    "#                 for x in bins1[i]:\n",
    "#                     error_vect[x]=0\n",
    "#             else:\n",
    "#                 for x in bins1[i]:\n",
    "#                     error_vect[x]=bin_error[i]\n",
    "#                 for x in bins0[i]:\n",
    "#                     error_vect[x]=0\n",
    "                    \n",
    "# ################ continue #########################################\n",
    "\n",
    "#         error_max = error_vect.max()\n",
    "\n",
    "#         if error_max != 0.:\n",
    "#             error_vect /= error_max\n",
    "\n",
    "#         if self.loss == 'square':\n",
    "#             error_vect **= 2\n",
    "#         elif self.loss == 'exponential':\n",
    "#             error_vect = 1. - np.exp(- error_vect)\n",
    "\n",
    "#         # Calculate the average loss\n",
    "#         estimator_error = (sample_weight * error_vect).sum()\n",
    "        \n",
    "#         if estimator_error <= 0:\n",
    "#             # Stop if fit is perfect\n",
    "#             return sample_weight, 1., 0.\n",
    "\n",
    "#         elif estimator_error >= 0.5:\n",
    "#             # Discard current estimator only if it isn't the only one\n",
    "#             if len(self.estimators_) > 1:\n",
    "#                 self.estimators_.pop(-1)\n",
    "#             return None, None, None\n",
    "        \n",
    "\n",
    "#         beta = estimator_error / (1. - estimator_error)\n",
    "\n",
    "#         # Boost weight using AdaBoost.R2 alg\n",
    "#         estimator_weight = self.learning_rate * np.log(1. / beta)\n",
    "#         #if this isnt the last iteration,\n",
    "#         if not iboost == self.n_estimators - 1:\n",
    "#             sample_weight *= np.power(\n",
    "#                 beta,\n",
    "#                 (1. - error_vect) * self.learning_rate)\n",
    "\n",
    "#         return sample_weight, estimator_weight, estimator_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "__all__ = [\n",
    "    'AdaBoostRegressorCK',\n",
    "]\n",
    "\n",
    "\n",
    "class BaseWeightBoostingCK(six.with_metaclass(ABCMeta, BaseEnsemble)):\n",
    "    \"\"\"Base class for AdaBoost estimators.\n",
    "    Warning: This class should not be used directly. Use derived classes\n",
    "    instead.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=50,\n",
    "                 estimator_params=tuple(),\n",
    "                 learning_rate=1.,\n",
    "                 random_state=None,\n",
    "                 g0=[],\n",
    "                 g1=[]):\n",
    "\n",
    "        super(BaseWeightBoostingCK, self).__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            estimator_params=estimator_params)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Build a boosted classifier/regressor from the training set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. COO, DOK, and LIL are converted to CSR. The dtype is\n",
    "            forced to DTYPE from tree._tree if the base classifier of this\n",
    "            ensemble weighted boosting classifier is a tree or forest.\n",
    "        y : array-like of shape = [n_samples]\n",
    "            The target values (class labels in classification, real numbers in\n",
    "            regression).\n",
    "        sample_weight : array-like of shape = [n_samples], optional\n",
    "            Sample weights. If None, the sample weights are initialized to\n",
    "            1 / n_samples.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # Check parameters\n",
    "        if self.learning_rate <= 0:\n",
    "            raise ValueError(\"learning_rate must be greater than zero\")\n",
    "\n",
    "        if (self.base_estimator is None or\n",
    "                isinstance(self.base_estimator, (BaseDecisionTree,\n",
    "                                                 BaseForest))):\n",
    "            dtype = DTYPE\n",
    "            accept_sparse = 'csc'\n",
    "        else:\n",
    "            dtype = None\n",
    "            accept_sparse = ['csr', 'csc']\n",
    "\n",
    "        X, y = check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype,\n",
    "                         y_numeric=is_regressor(self))\n",
    "\n",
    "        if sample_weight is None:\n",
    "            # Initialize weights to 1 / n_samples\n",
    "            sample_weight = np.empty(X.shape[0], dtype=np.float64)\n",
    "            sample_weight[:] = 1. / X.shape[0]\n",
    "        else:\n",
    "            sample_weight = check_array(sample_weight, ensure_2d=False)\n",
    "            # Normalize existing weights\n",
    "            sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)\n",
    "\n",
    "            # Check that the sample weights sum is positive\n",
    "            if sample_weight.sum() <= 0:\n",
    "                raise ValueError(\n",
    "                    \"Attempting to fit with a non-positive \"\n",
    "                    \"weighted number of samples.\")\n",
    "\n",
    "        # Check parameters\n",
    "        self._validate_estimator()\n",
    "\n",
    "        # Clear any previous fit results\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)\n",
    "        self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)\n",
    "\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        for iboost in range(self.n_estimators):\n",
    "            # Boosting step\n",
    "            sample_weight, estimator_weight, estimator_error = self._boost(\n",
    "                iboost,\n",
    "                X, y,\n",
    "                sample_weight,\n",
    "                random_state)\n",
    "\n",
    "            # Early termination\n",
    "            if sample_weight is None:\n",
    "                break\n",
    "\n",
    "            self.estimator_weights_[iboost] = estimator_weight\n",
    "            self.estimator_errors_[iboost] = estimator_error\n",
    "\n",
    "            # Stop if error is zero\n",
    "            if estimator_error == 0:\n",
    "                break\n",
    "\n",
    "            sample_weight_sum = np.sum(sample_weight)\n",
    "\n",
    "            # Stop if the sum of sample weights has become non-positive\n",
    "            if sample_weight_sum <= 0:\n",
    "                break\n",
    "\n",
    "            if iboost < self.n_estimators - 1:\n",
    "                # Normalize\n",
    "                sample_weight /= sample_weight_sum\n",
    "\n",
    "        return self\n",
    "\n",
    "    @abstractmethod\n",
    "    def _boost(self, iboost, X, y, sample_weight, random_state):\n",
    "        \"\"\"Implement a single boost.\n",
    "        Warning: This method needs to be overridden by subclasses.\n",
    "        Parameters\n",
    "        ----------\n",
    "        iboost : int\n",
    "            The index of the current boost iteration.\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
    "        y : array-like of shape = [n_samples]\n",
    "            The target values (class labels).\n",
    "        sample_weight : array-like of shape = [n_samples]\n",
    "            The current sample weights.\n",
    "        random_state : numpy.RandomState\n",
    "            The current random number generator\n",
    "        Returns\n",
    "        -------\n",
    "        sample_weight : array-like of shape = [n_samples] or None\n",
    "            The reweighted sample weights.\n",
    "            If None then boosting has terminated early.\n",
    "        estimator_weight : float\n",
    "            The weight for the current boost.\n",
    "            If None then boosting has terminated early.\n",
    "        error : float\n",
    "            The classification error for the current boost.\n",
    "            If None then boosting has terminated early.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def staged_score(self, X, y, sample_weight=None):\n",
    "        \"\"\"Return staged scores for X, y.\n",
    "        This generator method yields the ensemble score after each iteration of\n",
    "        boosting and therefore allows monitoring, such as to determine the\n",
    "        score on a test set after each boost.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. DOK and LIL are converted to CSR.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Labels for X.\n",
    "        sample_weight : array-like, shape = [n_samples], optional\n",
    "            Sample weights.\n",
    "        Returns\n",
    "        -------\n",
    "        z : float\n",
    "        \"\"\"\n",
    "        for y_pred in self.staged_predict(X):\n",
    "            if is_classifier(self):\n",
    "                yield accuracy_score(y, y_pred, sample_weight=sample_weight)\n",
    "            else:\n",
    "                yield r2_score(y, y_pred, sample_weight=sample_weight)\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        \"\"\"Return the feature importances (the higher, the more important the\n",
    "           feature).\n",
    "        Returns\n",
    "        -------\n",
    "        feature_importances_ : array, shape = [n_features]\n",
    "        \"\"\"\n",
    "        if self.estimators_ is None or len(self.estimators_) == 0:\n",
    "            raise ValueError(\"Estimator not fitted, \"\n",
    "                             \"call `fit` before `feature_importances_`.\")\n",
    "\n",
    "        try:\n",
    "            norm = self.estimator_weights_.sum()\n",
    "            return (sum(weight * clf.feature_importances_ for weight, clf\n",
    "                    in zip(self.estimator_weights_, self.estimators_))\n",
    "                    / norm)\n",
    "\n",
    "        except AttributeError:\n",
    "            raise AttributeError(\n",
    "                \"Unable to compute feature importances \"\n",
    "                \"since base_estimator does not have a \"\n",
    "                \"feature_importances_ attribute\")\n",
    "\n",
    "    def _validate_X_predict(self, X):\n",
    "        \"\"\"Ensure that X is in the proper format\"\"\"\n",
    "        if (self.base_estimator is None or\n",
    "                isinstance(self.base_estimator,\n",
    "                           (BaseDecisionTree, BaseForest))):\n",
    "            X = check_array(X, accept_sparse='csr', dtype=DTYPE)\n",
    "\n",
    "        else:\n",
    "            X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "def _samme_proba(estimator, n_classes, X):\n",
    "    \"\"\"Calculate algorithm 4, step 2, equation c) of Zhu et al [1].\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
    "    \"\"\"\n",
    "    proba = estimator.predict_proba(X)\n",
    "\n",
    "    # Displace zero probabilities so the log is defined.\n",
    "    # Also fix negative elements which may occur with\n",
    "    # negative sample weights.\n",
    "    proba[proba < np.finfo(proba.dtype).eps] = np.finfo(proba.dtype).eps\n",
    "    log_proba = np.log(proba)\n",
    "\n",
    "    return (n_classes - 1) * (log_proba - (1. / n_classes)\n",
    "                              * log_proba.sum(axis=1)[:, np.newaxis])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(print \"start boosting!\")? (<ipython-input-1-579973cff8ac>, line 147)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-579973cff8ac>\"\u001b[0;36m, line \u001b[0;32m147\u001b[0m\n\u001b[0;31m    print \"start boosting!\"\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(print \"start boosting!\")?\n"
     ]
    }
   ],
   "source": [
    "class AdaBoostRegressorCK(BaseWeightBoostingCK, RegressorMixin):\n",
    "    \"\"\"An AdaBoost regressor.\n",
    "    An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\n",
    "    regressor on the original dataset and then fits additional copies of the\n",
    "    regressor on the same dataset but where the weights of instances are\n",
    "    adjusted according to the error of the current prediction. As such,\n",
    "    subsequent regressors focus more on difficult cases.\n",
    "    This class implements the algorithm known as AdaBoost.R2 [2].\n",
    "    Read more in the :ref:`User Guide <adaboost>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimator : object, optional (default=DecisionTreeRegressor)\n",
    "        The base estimator from which the boosted ensemble is built.\n",
    "        Support for sample weighting is required.\n",
    "    n_estimators : integer, optional (default=50)\n",
    "        The maximum number of estimators at which boosting is terminated.\n",
    "        In case of perfect fit, the learning procedure is stopped early.\n",
    "    learning_rate : float, optional (default=1.)\n",
    "        Learning rate shrinks the contribution of each regressor by\n",
    "        ``learning_rate``. There is a trade-off between ``learning_rate`` and\n",
    "        ``n_estimators``.\n",
    "    loss : {'linear', 'square', 'exponential'}, optional (default='linear')\n",
    "        The loss function to use when updating the weights after each\n",
    "        boosting iteration.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "    Attributes\n",
    "    ----------\n",
    "    estimators_ : list of classifiers\n",
    "        The collection of fitted sub-estimators.\n",
    "    estimator_weights_ : array of floats\n",
    "        Weights for each estimator in the boosted ensemble.\n",
    "    estimator_errors_ : array of floats\n",
    "        Regression error for each estimator in the boosted ensemble.\n",
    "    feature_importances_ : array of shape = [n_features]\n",
    "        The feature importances if supported by the ``base_estimator``.\n",
    "    See also\n",
    "    --------\n",
    "    AdaBoostClassifier, GradientBoostingRegressor, DecisionTreeRegressor\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
    "           on-Line Learning and an Application to Boosting\", 1995.\n",
    "    .. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #we pass group matrices to method: g1,g2\n",
    "    #g1:[k][n] matrix - each row is indices of points in this bin\n",
    "    #g2:[k][m] matrix - numberof bins is the same but number of instances in each bin may vary between groups\n",
    "    def __init__(self,\n",
    "                 base_estimator=None,\n",
    "                 n_estimators=50,\n",
    "                 learning_rate=1.,\n",
    "                 loss='linear',\n",
    "                 random_state=None,\n",
    "                 g1=[],\n",
    "                 g2=[]):\n",
    "\n",
    "        super(AdaBoostRegressorCK, self).__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            random_state=random_state)\n",
    "\n",
    "        self.loss = loss\n",
    "        self.random_state = random_state\n",
    "        \n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Build a boosted regressor from the training set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. DOK and LIL are converted to CSR.\n",
    "        y : array-like of shape = [n_samples]\n",
    "            The target values (real numbers).\n",
    "        sample_weight : array-like of shape = [n_samples], optional\n",
    "            Sample weights. If None, the sample weights are initialized to\n",
    "            1 / n_samples.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # Check loss\n",
    "        if self.loss not in ('linear', 'square', 'exponential'):\n",
    "            raise ValueError(\n",
    "                \"loss must be 'linear', 'square', or 'exponential'\")\n",
    "\n",
    "        # Fit\n",
    "        return super(AdaBoostRegressorCK, self).fit(X, y, sample_weight)\n",
    "\n",
    "    def _validate_estimator(self):\n",
    "        \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n",
    "        super(AdaBoostRegressorCK, self)._validate_estimator(\n",
    "            default=DecisionTreeRegressor(max_depth=3))\n",
    "        \n",
    "        \n",
    "        \n",
    "#########################################################################################\n",
    "def _boost(self, iboost, X, y, sample_weight, random_state):\n",
    "        \"\"\"Implement a single boost for regression\n",
    "        Perform a single boost according to the AdaBoost.R2 algorithm and\n",
    "        return the updated sample weights.\n",
    "        Parameters\n",
    "        ----------\n",
    "        iboost : int\n",
    "            The index of the current boost iteration.\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "            DOK, or LIL. DOK and LIL are converted to CSR.\n",
    "        y : array-like of shape = [n_samples]\n",
    "            The target values (class labels in classification, real numbers in\n",
    "            regression).\n",
    "        sample_weight : array-like of shape = [n_samples]\n",
    "            The current sample weights.\n",
    "        random_state : numpy.RandomState\n",
    "            The current random number generator\n",
    "        Returns\n",
    "        -------\n",
    "        sample_weight : array-like of shape = [n_samples] or None\n",
    "            The reweighted sample weights.\n",
    "            If None then boosting has terminated early.\n",
    "        estimator_weight : float\n",
    "            The weight for the current boost.\n",
    "            If None then boosting has terminated early.\n",
    "        estimator_error : float\n",
    "            The regression error for the current boost.\n",
    "            If None then boosting has terminated early.\n",
    "        \"\"\"\n",
    "        estimator = self._make_estimator(random_state=random_state)\n",
    "        \n",
    "        # Weighted sampling of the training set with replacement\n",
    "        # For NumPy >= 1.7.0 use np.random.choice\n",
    "        cdf = stable_cumsum(sample_weight)\n",
    "        cdf /= cdf[-1]\n",
    "        uniform_samples = random_state.random_sample(X.shape[0])\n",
    "        bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')\n",
    "        # searchsorted returns a scalar\n",
    "        bootstrap_idx = np.array(bootstrap_idx, copy=False)\n",
    "\n",
    "        print \"start boosting!\"\n",
    "        print\n",
    "        # Fit on the bootstrapped sample and obtain a prediction\n",
    "        # for all samples in the training set\n",
    "        estimator.fit(X[bootstrap_idx], y[bootstrap_idx])\n",
    "        y_predict = estimator.predict(X)\n",
    "        print \"Iteration\" + str(iboost) + str(mean_squared_error(y, y_predict))\n",
    "\n",
    "###########  REPLACE ERROR VECT WITH OUR OWN   ########\n",
    "#         error_vect = np.abs(y_predict - y)\n",
    "    \n",
    "################ compute bin-wise group error #####################\n",
    "\n",
    "        nbins=10\n",
    "        bins0,bins1  = recomputeBins(y_predict, g0, g1, nbins)\n",
    "        # sum the error for items in bin for each group\n",
    "        e0=[np.sum([error_vect[i] for i in b]) for b in bins0]\n",
    "        e1=[np.sum([error_vect[i] for i in b]) for b in bins1]\n",
    "\n",
    "        bin_error = np.subtract(e0, e1)\n",
    "        print \"errors: \", bin_error\n",
    "        ################# Update weights based on binned error ############\n",
    "\n",
    "        # weight proportional to difference between the error in each group in the bin. \n",
    "        # but could be based off of the individual error for each term\n",
    "        # or some combination\n",
    "        for i,e in enumerate(bin_error):\n",
    "            if e < 0:\n",
    "                for x in bins0[i]:\n",
    "                    error_vect[x]=bin_error[i]\n",
    "                for x in bins1[i]:\n",
    "                    error_vect[x]=0\n",
    "            else:\n",
    "                for x in bins1[i]:\n",
    "                    error_vect[x]=bin_error[i]\n",
    "                for x in bins0[i]:\n",
    "                    error_vect[x]=0\n",
    "                    \n",
    "################ continue #########################################\n",
    "\n",
    "        error_max = error_vect.max()\n",
    "\n",
    "        if error_max != 0.:\n",
    "            error_vect /= error_max\n",
    "\n",
    "        if self.loss == 'square':\n",
    "            error_vect **= 2\n",
    "        elif self.loss == 'exponential':\n",
    "            error_vect = 1. - np.exp(- error_vect)\n",
    "\n",
    "        # Calculate the average loss\n",
    "        estimator_error = (sample_weight * error_vect).sum()\n",
    "        \n",
    "        if estimator_error <= 0:\n",
    "            # Stop if fit is perfect\n",
    "            return sample_weight, 1., 0.\n",
    "\n",
    "        elif estimator_error >= 0.5:\n",
    "            # Discard current estimator only if it isn't the only one\n",
    "            if len(self.estimators_) > 1:\n",
    "                self.estimators_.pop(-1)\n",
    "            return None, None, None\n",
    "        \n",
    "\n",
    "        beta = estimator_error / (1. - estimator_error)\n",
    "\n",
    "        # Boost weight using AdaBoost.R2 alg\n",
    "        estimator_weight = self.learning_rate * np.log(1. / beta)\n",
    "        #if this isnt the last iteration,\n",
    "        if not iboost == self.n_estimators - 1:\n",
    "            sample_weight *= np.power(\n",
    "                beta,\n",
    "                (1. - error_vect) * self.learning_rate)\n",
    "\n",
    "        return sample_weight, estimator_weight, estimator_error\n",
    "    \n",
    "#########################################################################################\n",
    "\n",
    "        def _get_median_predict(self, X, limit):\n",
    "            # Evaluate predictions of all estimators\n",
    "            predictions = np.array([\n",
    "                est.predict(X) for est in self.estimators_[:limit]]).T\n",
    "\n",
    "            # Sort the predictions\n",
    "            sorted_idx = np.argsort(predictions, axis=1)\n",
    "\n",
    "            # Find index of median prediction for each sample\n",
    "            weight_cdf = stable_cumsum(self.estimator_weights_[sorted_idx], axis=1)\n",
    "            median_or_above = weight_cdf >= 0.5 * weight_cdf[:, -1][:, np.newaxis]\n",
    "            median_idx = median_or_above.argmax(axis=1)\n",
    "\n",
    "            median_estimators = sorted_idx[np.arange(X.shape[0]), median_idx]\n",
    "\n",
    "            # Return median predictions\n",
    "            return predictions[np.arange(X.shape[0]), median_estimators]\n",
    "\n",
    "        def predict(self, X):\n",
    "            \"\"\"Predict regression value for X.\n",
    "            The predicted regression value of an input sample is computed\n",
    "            as the weighted median prediction of the classifiers in the ensemble.\n",
    "            Parameters\n",
    "            ----------\n",
    "            X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "                The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "                DOK, or LIL. DOK and LIL are converted to CSR.\n",
    "            Returns\n",
    "            -------\n",
    "            y : array of shape = [n_samples]\n",
    "                The predicted regression values.\n",
    "            \"\"\"\n",
    "            check_is_fitted(self, \"estimator_weights_\")\n",
    "            X = self._validate_X_predict(X)\n",
    "\n",
    "            return self._get_median_predict(X, len(self.estimators_))\n",
    "\n",
    "        def staged_predict(self, X):\n",
    "            \"\"\"Return staged predictions for X.\n",
    "            The predicted regression value of an input sample is computed\n",
    "            as the weighted median prediction of the classifiers in the ensemble.\n",
    "            This generator method yields the ensemble prediction after each\n",
    "            iteration of boosting and therefore allows monitoring, such as to\n",
    "            determine the prediction on a test set after each boost.\n",
    "            Parameters\n",
    "            ----------\n",
    "            X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "                The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
    "                DOK, or LIL. DOK and LIL are converted to CSR.\n",
    "            Returns\n",
    "            -------\n",
    "            y : generator of array, shape = [n_samples]\n",
    "                The predicted regression values.\n",
    "            \"\"\"\n",
    "            check_is_fitted(self, \"estimator_weights_\")\n",
    "            X = self._validate_X_predict(X)\n",
    "\n",
    "            for i, _ in enumerate(self.estimators_, 1):\n",
    "                yield self._get_median_predict(X, limit=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "X = pickle.load( open( \"x.p\", \"rb\" ) )\n",
    "y = pickle.load( open( \"y.p\", \"rb\" ) )\n",
    "g = pickle.load( open( \"g.p\", \"rb\" ) )\n",
    "g0 = [i for i,x in enumerate(g) if x==0]\n",
    "g1 = [i for i,x in enumerate(g) if x==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# dt = DecisionTreeRegressor(max_depth=4)\n",
    "abt = AdaBoostRegressorCK(DecisionTreeRegressor(max_depth=4),n_estimators=10, random_state=25)\n",
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validation:\n",
    "# pred_dt = cross_val_predict(dt, X, y, cv=10)\n",
    "pred_abt = cross_val_predict(abt, X, y, cv=10)\n",
    "# print(\"Overall MSE: \",  mean_squared_error(y, pred_dt))\n",
    "print(\"Overall MSE: \",  mean_squared_error(y, pred_abt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############## TESTING ######################\n",
    "\n",
    "# from sklearn.ensemble import AdaBoostRegressor\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.cross_validation import cross_val_predict\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# dt = DecisionTreeRegressor(max_depth=4)\n",
    "# abt = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),n_estimators=300, random_state=25)\n",
    "# # cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# # is a prediction obtained by cross validation:\n",
    "# pred_dt = cross_val_predict(dt, X, y, cv=10)\n",
    "# pred_abt = cross_val_predict(abt, X, y, cv=10)\n",
    "# print(\"Overall MSE: \",  mean_squared_error(y, pred_dt))\n",
    "# print(\"Overall MSE: \",  mean_squared_error(y, pred_abt))\n",
    "\n",
    "# estimator = DecisionTreeRegressor(max_depth=4)\n",
    "# # Initialize weights to 1 / n_samples\n",
    "# sample_weight = np.empty(X.shape[0], dtype=np.float64)\n",
    "# sample_weight[:] = 1. / X.shape[0]        \n",
    "# cdf = stable_cumsum(sample_weight)\n",
    "# cdf /= cdf[-1]\n",
    "# uniform_samples = np.random.random_sample(X.shape[0])\n",
    "# bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')\n",
    "# # searchsorted returns a scalar\n",
    "# bootstrap_idx = np.array(bootstrap_idx, copy=False)\n",
    "\n",
    "# # Fit on the bootstrapped sample and obtain a prediction\n",
    "# # for all samples in the training set\n",
    "# estimator.fit(X[bootstrap_idx], y[bootstrap_idx])\n",
    "# y_predict = estimator.predict(X)\n",
    "# error_vect = np.abs(y_predict - y)\n",
    "# error_max = error_vect.max()\n",
    "# if error_max != 0.:\n",
    "#     error_vect /= error_max\n",
    "    \n",
    "# ################ compute bin-wise group error #####################\n",
    "\n",
    "# nbins=10\n",
    "# bins0,bins1  = recomputeBins(y_predict, g0, g1, nbins)\n",
    "# # sum the error for items in bin for each group\n",
    "# e0=[np.sum([error_vect[i] for i in b]) for b in bins0]\n",
    "# e1=[np.sum([error_vect[i] for i in b]) for b in bins1]\n",
    "\n",
    "# bin_error = np.subtract(e0, e1)\n",
    "\n",
    "# ################# Update weights based on binned error ############\n",
    "\n",
    "# # weight proportional to difference between the error in each group in the bin. \n",
    "# # but could be based off of the individual error for each term\n",
    "# # or some combination\n",
    "# for i,e in enumerate(bin_error):\n",
    "#     if e < 0:\n",
    "#         for x in bins0[i]:\n",
    "#             error_vect[x]=bin_error[i]\n",
    "#         for x in bins1[i]:\n",
    "#             error_vect[x]=0\n",
    "#     else:\n",
    "#         for x in bins1[i]:\n",
    "#             error_vect[x]=bin_error[i]\n",
    "#         for x in bins0[i]:\n",
    "#             error_vect[x]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#######3#### DEBUG binning ############ \n",
    "\n",
    "\n",
    "# # print error_vect\n",
    "# # weight proportional to difference between the error in each group in the bin. \n",
    "# # but could be based off of the individual error for each term\n",
    "# # or some combination\n",
    "# for i,e in enumerate(bin_error):\n",
    "#     if e < 0:\n",
    "#         #weight instances in g1\n",
    "#         for x in bins0[i]:\n",
    "# #             print \"G0 change \"+ str(x)+\" to num :\"+str(e) \n",
    "#             error_vect[x]=bin_error[i]\n",
    "#         for x in bins1[i]:\n",
    "# #             print \"G0 change to 0 :\" + str(x)\n",
    "#             error_vect[x]=0\n",
    "# #         print error_vect\n",
    "#     else:\n",
    "#         #weight instances in g0\n",
    "#         for x in bins1[i]:\n",
    "# #             print \"G1 change to num :\" + str(x)\n",
    "#             error_vect[x]=bin_error[i]\n",
    "#         for x in bins0[i]:\n",
    "# #             print \"G1 change to 0 :\" + str(x)\n",
    "#             error_vect[x]=0\n",
    "# #         print error_vect"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
